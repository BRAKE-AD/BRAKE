{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdwcO26eLp08"
      },
      "source": [
        "## Raw Readers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "folder = \"/Users/gabriele/Desktop/Magistrale/Explainable_and_trustworthy_AI/progetti/concept_gridlock/kaggle/working/road-save/comma/cache/resnet50I3D512-Pkinetics-b4s8x1x1-commat3-h3x3x3\"\n",
        "\n",
        "if not os.path.exists(folder):\n",
        "    print(f\"❌ The folder '{folder}' does not exist!\")\n",
        "else:\n",
        "    print(f\"📂 Scanning folder: {folder}\")\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        print(f\"🔎 Found file: {filename}\")  # debug\n",
        "\n",
        "        if \"_240frames\" in filename:\n",
        "            new_name = filename.replace(\"_240frames\", \"\")\n",
        "            old_path = os.path.join(folder, filename)\n",
        "            new_path = os.path.join(folder, new_name)\n",
        "\n",
        "            os.rename(old_path, new_path)\n",
        "            print(f\"✅ Renamed: {filename} → {new_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caPmSFqoOgd7",
        "outputId": "eb2ec45e-0838-4364-8d66-0b69d25073a9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Gabriele-Raffaele/openpilot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajQ6pY2DbLdG",
        "outputId": "abdc0923-549c-47f9-e73d-2cf30740de2b"
      },
      "outputs": [],
      "source": [
        "!pip install pycapnp\n",
        "!pip install smbus2\n",
        "!pip install lru-dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAna3ipRVtbb",
        "outputId": "35412cab-2da4-4200-963d-dd066a3d0ddd"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import platform\n",
        "platform.architecture()\n",
        "sys.path.append(\"/content/openpilot\")\n",
        "from tools.lib.logreader import LogReader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tools.lib.framereader import FrameReader\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBrZDaWVcs7k",
        "outputId": "1c1c19ff-16ac-48e9-9249-da1116fa3971"
      },
      "outputs": [],
      "source": [
        "!git clone \"https://github.com/commaai/comma2k19.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E770l3VajnQb",
        "outputId": "7ab835f4-d817-4ad0-c93f-f2af8e0e9c9a"
      },
      "outputs": [],
      "source": [
        "# Download first chunk (~9GB)\n",
        "!wget https://huggingface.co/datasets/commaai/comma2k19/resolve/431c287f12295222eb427a9cff821d63101f2169/Chunk_1.zip -O Chunk_1.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps1QT-kgEYua"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset\n",
        "!mv Chunk_1.zip /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "35cEY7mNk5U4",
        "outputId": "85212e17-df93-4469-f91c-8132d0b8489a"
      },
      "outputs": [],
      "source": [
        "!unzip /content/dataset/Chunk_1.zip -d /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGheqnW3C8V4"
      },
      "outputs": [],
      "source": [
        "removed_files = []\n",
        "def get_sample(p):\n",
        "    frame_reader = FrameReader(p+'/video.hevc')\n",
        "    logs = list(LogReader(p + '/raw_log.bz2'))\n",
        "\n",
        "    angle = np.array([l.carState.steeringAngleDeg for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    time = np.array([l.logMonoTime for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    vEgo = np.array([l.carState.vEgo for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    gas = np.array([l.carState.gas for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    brake = np.array([l.carState.brake for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    gps_times = np.load(p + '/global_pose/frame_gps_times')\n",
        "    times = np.load(p + '/global_pose/frame_times')\n",
        "    gas = np.array([l.carState.gas for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    gaspressed = np.array([l.carState.gasPressed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    brake = np.array([l.carState.brake for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    brake_pressed = np.array([l.carState.brakePressed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "    enabled = np.array([l.carState.cruiseState.enabled for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    speed = np.array([l.carState.cruiseState.speed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    speedOffset = np.array([l.carState.cruiseState.speedOffset for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    standstill = np.array([l.carState.cruiseState.standstill for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    nonAdaptive = np.array([l.carState.cruiseState.nonAdaptive for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    speedCluster = np.array([l.carState.cruiseState.speedCluster for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "    leftBlinker = np.array([l.carState.leftBlinker for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    rightBlinker = np.array([l.carState.rightBlinker for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    #print(frame_reader.frame_count, gps_times.shape, times.shape)\n",
        "    #print([l.carState for l in logs if l.which() == \"carState\"][0])\n",
        "    #print([l.radarState for l in logs if l.which() == \"radarState\"][0])\n",
        "\n",
        "    dist = np.array([l.radarState.leadOne.dRel for l in logs if l.which() == \"radarState\"])[1::5]\n",
        "    if ((vEgo == 0).mean() > 0.2) or ((dist == 0).mean() > 0.2) or len(dist) <=230:\n",
        "      print(f\"❌ Removed {p} because vEgo/dist too low, vEgo_zeros={(vEgo==0).mean():.2f}, dist_zeros={(dist==0).mean():.2f}, len(dist)={len(dist)}\\n\")\n",
        "      removed_files.append(p)\n",
        "      return None\n",
        "    images = []\n",
        "    l = list(range(frame_reader.frame_count))\n",
        "    if len(l) > 245:\n",
        "        l = l[1::5]\n",
        "    for idx in list(range(frame_reader.frame_count))[1::5]:\n",
        "        image = np.array(frame_reader.get(idx, pix_fmt='rgb24')[0], dtype=np.float64)\n",
        "        image = cv2.resize(image, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "        images.append(image)\n",
        "    steady_state = ~gaspressed & ~brake_pressed & ~leftBlinker & ~rightBlinker\n",
        "    last_idx = 0\n",
        "    desired_gap = np.zeros(steady_state.shape)\n",
        "\n",
        "    for i in range(len(steady_state)-1):\n",
        "        if steady_state[i] == True:\n",
        "            desired_gap[last_idx:i] = int(dist[i])\n",
        "            last_idx = i\n",
        "\n",
        "    #VIDEO_ID feature\n",
        "    #parent_folder = os.path.basename(os.path.dirname(p))  # 'b0c9d2329ad1606b|2018-07-27--06-03-57'\n",
        "    #sub_folder = os.path.basename(p)                      # '10'\n",
        "    #print(f\"p: {p}\")\n",
        "    #print(f\"parent_folder: {parent_folder}\")\n",
        "    #print(f\"sub_folder: {sub_folder}\")\n",
        "\n",
        "    #video_id = f\"{parent_folder}_{sub_folder}\"\n",
        "    #print(video_id)\n",
        "    sample = {\n",
        "        #'video_id': video_id,\n",
        "        'image': images,\n",
        "        \"CruiseStateenabled\": enabled,\n",
        "        \"CruiseStatespeed\": speed,\n",
        "        \"CruiseStatespeedOffset\": speedOffset,\n",
        "        \"CruiseStatestandstill\": standstill,\n",
        "        \"CruiseStatenonAdaptive\": nonAdaptive,\n",
        "        \"CruiseStatespeedCluster\": speedCluster,\n",
        "        'leftBlinker': leftBlinker,\n",
        "        'rightBlinker': rightBlinker,\n",
        "        \"gas\": gas,\n",
        "        \"gaspressed\": gaspressed,\n",
        "        \"brake\": brake,\n",
        "        \"brakepressed\": brake_pressed,\n",
        "        'angle': angle,\n",
        "        'time': time,\n",
        "        'gas': gas,\n",
        "        'vEgo': vEgo,\n",
        "        'brake': brake,\n",
        "        'dist': dist,\n",
        "        'desired_dist': desired_gap,\n",
        "        }\n",
        "    if (desired_gap == 0).mean() > 0.2:\n",
        "      print(f\"❌ Discarded {p} because desired_gap too low {(desired_gap == 0).mean():.2f}\")\n",
        "      removed_files.append(p)\n",
        "      return None\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "removed_files = []\n",
        "def get_sample_2(p):\n",
        "    frame_reader = FrameReader(p+'/video.hevc')\n",
        "    logs = list(LogReader(p + '/raw_log.bz2'))\n",
        "\n",
        "    angle = np.array([l.carState.steeringAngleDeg for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    time = np.array([l.logMonoTime for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    vEgo = np.array([l.carState.vEgo for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    gas = np.array([l.carState.gas for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    brake = np.array([l.carState.brake for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    gps_times = np.load(p + '/global_pose/frame_gps_times')\n",
        "    times = np.load(p + '/global_pose/frame_times')\n",
        "    gas = np.array([l.carState.gas for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    gaspressed = np.array([l.carState.gasPressed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    brake = np.array([l.carState.brake for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    brake_pressed = np.array([l.carState.brakePressed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "    enabled = np.array([l.carState.cruiseState.enabled for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    speed = np.array([l.carState.cruiseState.speed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    speedOffset = np.array([l.carState.cruiseState.speedOffset for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    standstill = np.array([l.carState.cruiseState.standstill for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    nonAdaptive = np.array([l.carState.cruiseState.nonAdaptive for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    speedCluster = np.array([l.carState.cruiseState.speedCluster for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "    leftBlinker = np.array([l.carState.leftBlinker for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    rightBlinker = np.array([l.carState.rightBlinker for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "    #print(frame_reader.frame_count, gps_times.shape, times.shape)\n",
        "    #print([l.carState for l in logs if l.which() == \"carState\"][0])\n",
        "    #print([l.radarState for l in logs if l.which() == \"radarState\"][0])\n",
        "\n",
        "    dist = np.array([l.radarState.leadOne.dRel for l in logs if l.which() == \"radarState\"])[1::5]\n",
        "    if ((vEgo == 0).mean() > 0.2) or ((dist == 0).mean() > 0.2) or len(dist) <=230:\n",
        "     \n",
        "      removed_files.append(p)\n",
        "      return None\n",
        "    \n",
        "    steady_state = ~gaspressed & ~brake_pressed & ~leftBlinker & ~rightBlinker\n",
        "    last_idx = 0\n",
        "    desired_gap = np.zeros(steady_state.shape)\n",
        "\n",
        "    for i in range(len(steady_state)-1):\n",
        "        if steady_state[i] == True:\n",
        "            desired_gap[last_idx:i] = int(dist[i])\n",
        "            last_idx = i\n",
        "\n",
        "    #VIDEO_ID feature\n",
        "    #parent_folder = os.path.basename(os.path.dirname(p))  # 'b0c9d2329ad1606b|2018-07-27--06-03-57'\n",
        "    #sub_folder = os.path.basename(p)                      # '10'\n",
        "    #print(f\"p: {p}\")\n",
        "    #print(f\"parent_folder: {parent_folder}\")\n",
        "    #print(f\"sub_folder: {sub_folder}\")\n",
        "\n",
        "    #video_id = f\"{parent_folder}_{sub_folder}\"\n",
        "    #print(video_id)\n",
        "    sample = {\n",
        "        #'video_id': video_id,\n",
        "        \n",
        "        \"CruiseStateenabled\": enabled,\n",
        "        \"CruiseStatespeed\": speed,\n",
        "        \"CruiseStatespeedOffset\": speedOffset,\n",
        "        \"CruiseStatestandstill\": standstill,\n",
        "        \"CruiseStatenonAdaptive\": nonAdaptive,\n",
        "        \"CruiseStatespeedCluster\": speedCluster,\n",
        "        'leftBlinker': leftBlinker,\n",
        "        'rightBlinker': rightBlinker,\n",
        "        \"gas\": gas,\n",
        "        \"gaspressed\": gaspressed,\n",
        "        \"brake\": brake,\n",
        "        \"brakepressed\": brake_pressed,\n",
        "        'angle': angle,\n",
        "        'time': time,\n",
        "        'gas': gas,\n",
        "        'vEgo': vEgo,\n",
        "        'brake': brake,\n",
        "        'dist': dist,\n",
        "        'desired_dist': desired_gap,\n",
        "        }\n",
        "    if (desired_gap == 0).mean() > 0.2:\n",
        "      \n",
        "      removed_files.append(p)\n",
        "      return None\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8iCEQ_RC8TP"
      },
      "outputs": [],
      "source": [
        "def save_h5py(i, sample, h):\n",
        "    group = h.create_group(str(i))\n",
        "    for col in sample.keys():\n",
        "            dt = np.float32 if col != 'image' else int#\n",
        "            dataset_name = col #groups are divided by '/'\n",
        "            a = list(sample[col])\n",
        "            group.create_dataset(dataset_name, data = np.asarray(a, dtype=dt),\n",
        "                    #compression_opts=9,\n",
        "                    #chunks=(164, 20, 20, 3),\n",
        "                    compression='lzf')\n",
        "\n",
        "#Use this if you want to save also video_id as feature\n",
        "'''def save_h5py(i, sample, h):\n",
        "    group = h.create_group(str(i))\n",
        "    for col in sample.keys():\n",
        "        if col == \"video_id\":\n",
        "            dt = h5py.string_dtype(encoding='utf-8')\n",
        "            group.create_dataset(col, data=sample[col], dtype=dt)\n",
        "        else:\n",
        "            dt = np.float32 if col != 'image' else int\n",
        "            a = list(sample[col])\n",
        "            group.create_dataset(col, data=np.asarray(a, dtype=dt), compression='lzf')'''\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyZzyPpl3CO9"
      },
      "outputs": [],
      "source": [
        "!rm -r \"/content/gas_and_brake_train_comma_chunk_1_w_imgs.hdf5\"\n",
        "!rm -r \"/content/gas_and_brake_test_comma_chunk_1_w_imgs.hdf5\"\n",
        "!rm -r \"/content/gas_and_brake_val_comma_chunk_1_w_imgs.hdf5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtJRHKtjAzZg"
      },
      "outputs": [],
      "source": [
        "main_dir='/content/dataset/Chunk_1_reduced/'\n",
        "hdf5_filename = \"gas_and_brake_train_comma_chunk_1_w_imgs.hdf5\"\n",
        "h_train = h5py.File(hdf5_filename, 'w')\n",
        "hdf5_filename = \"gas_and_brake_val_comma_chunk_1_w_imgs.hdf5\"\n",
        "h_val = h5py.File(hdf5_filename, 'w')\n",
        "hdf5_filename = \"gas_and_brake_test_comma_chunk_1_w_imgs.hdf5\"\n",
        "h_test = h5py.File(hdf5_filename, 'w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEc4jw_AA6xS",
        "outputId": "fe6b0bc6-be64-421a-96d5-ac08b03215d9"
      },
      "outputs": [],
      "source": [
        "#Old version\n",
        "for j, drive_sequence_path in tqdm(enumerate(os.listdir(main_dir))):\n",
        "    if '.DS_Store ' in drive_sequence_path or not os.path.isdir(main_dir+\"/\"+drive_sequence_path):\n",
        "      print(f\"⚠️ Skipping non-directory or DS_Store: {drive_sequence_path}\")\n",
        "      continue\n",
        "    min_sequence_paths = os.listdir(main_dir+\"/\"+drive_sequence_path)\n",
        "    if len(min_sequence_paths) < 3:\n",
        "      print(f\"⚠️ Skipping {drive_sequence_path} because minor than 3 subdirectory \")\n",
        "      continue\n",
        "    min_sequence_path_test = main_dir+\"/\"+drive_sequence_path+\"/\"+min_sequence_paths[-2]\n",
        "    min_sequence_paths_val = main_dir+\"/\"+drive_sequence_path+\"/\"+min_sequence_paths[-1]\n",
        "    sample = get_sample(min_sequence_path_test)\n",
        "    if sample != None:\n",
        "        save_h5py(f\"{drive_sequence_path}\", sample, h_test)\n",
        "        print(f\"✅ Test saved: {drive_sequence_path}\")\n",
        "    else:\n",
        "        print(f\"❌ Test discarded: {min_sequence_path_test}\")\n",
        "\n",
        "    sample = get_sample(min_sequence_paths_val)\n",
        "\n",
        "    if sample != None:\n",
        "        save_h5py(f\"{drive_sequence_path}\", sample, h_val)\n",
        "        print(f\"✅ Val saved: {drive_sequence_path}\")\n",
        "    else:\n",
        "        print(f\"❌ Val discarded: {min_sequence_path_test}\")\n",
        "\n",
        "    for i, min_sequence in enumerate(min_sequence_paths[:-1]):\n",
        "        if '.DS_Store ' in min_sequence or not os.path.isdir(main_dir+\"/\"+drive_sequence_path+'/'+min_sequence):\n",
        "          print(f\"⚠️ Skipping train non-directory or DS_Store: {min_sequence}\")\n",
        "          continue\n",
        "        p = main_dir+\"/\"+drive_sequence_path+\"/\"+min_sequence\n",
        "        sample = get_sample(p)\n",
        "        if sample != None:\n",
        "            save_h5py(f\"{drive_sequence_path}_{min_sequence}\", sample, h_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "main_dir = \"/content/dataset/Chunk_1\"  # principal folder\n",
        "split_ratios = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15}\n",
        "\n",
        "# --- Collect all valid sequences ---\n",
        "all_sequences = []\n",
        "removed_sequences = []\n",
        "\n",
        "for drive_sequence_path in tqdm(os.listdir(main_dir), desc=\"Scanning drives\"):\n",
        "    full_drive_path = os.path.join(main_dir, drive_sequence_path)\n",
        "    if not os.path.isdir(full_drive_path) or \".DS_Store\" in drive_sequence_path:\n",
        "        continue\n",
        "\n",
        "    for seq in os.listdir(full_drive_path):\n",
        "        full_seq_path = os.path.join(full_drive_path, seq)\n",
        "        if os.path.isdir(full_seq_path):\n",
        "            # ⚠️ Check if the sequence is valid\n",
        "            sample = get_sample_2(full_seq_path)\n",
        "            if sample is not None:\n",
        "                all_sequences.append((drive_sequence_path, seq, full_seq_path))\n",
        "            else:\n",
        "                removed_sequences.append(full_seq_path)\n",
        "\n",
        "print(f\"📦 Found {len(all_sequences)} valid sequences\")\n",
        "print(f\"🗑️ Discarded {len(removed_sequences)} sequences\")\n",
        "\n",
        "# --- Shuffle ---\n",
        "random.seed(42)  # for reproducibility\n",
        "random.shuffle(all_sequences)\n",
        "\n",
        "# --- Split globale ---\n",
        "n_total = len(all_sequences)\n",
        "n_train = int(split_ratios[\"train\"] * n_total)\n",
        "n_val   = int(split_ratios[\"val\"] * n_total)\n",
        "# test = rest\n",
        "train_seqs = all_sequences[:n_train]\n",
        "val_seqs   = all_sequences[n_train:n_train+n_val]\n",
        "test_seqs  = all_sequences[n_train+n_val:]\n",
        "\n",
        "print(f\"👉 Train: {len(train_seqs)}, Val: {len(val_seqs)}, Test: {len(test_seqs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_seqs= [('b0c9d2329ad1606b|2018-08-10--22-42-26', '5', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-10--22-42-26/5'), ('b0c9d2329ad1606b|2018-07-30--13-03-07', '21', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-03-07/21'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '34', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/34'), ('b0c9d2329ad1606b|2018-08-17--14-55-39', '6', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-55-39/6'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '11', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/11'), ('b0c9d2329ad1606b|2018-08-17--14-17-47', '11', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-17-47/11'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '40', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/40'), ('b0c9d2329ad1606b|2018-08-06--10-04-53', '25', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-06--10-04-53/25'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '35', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/35'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '11', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/11'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '26', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/26'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '28', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/28'), ('b0c9d2329ad1606b|2018-08-17--14-17-47', '6', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-17-47/6'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '31', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/31'), ('b0c9d2329ad1606b|2018-08-10--22-42-26', '10', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-10--22-42-26/10'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '32', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/32'), ('b0c9d2329ad1606b|2018-08-10--22-42-26', '7', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-10--22-42-26/7'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '10', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/10'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '35', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/35'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '41', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/41'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '32', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/32'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '21', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/21'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '8', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/8'), ('b0c9d2329ad1606b|2018-08-10--22-42-26', '8', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-10--22-42-26/8'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '9', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/9'), ('b0c9d2329ad1606b|2018-07-29--11-17-20', '6', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-29--11-17-20/6'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '4', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/4'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '12', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/12'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '13', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/13'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '29', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/29'), ('b0c9d2329ad1606b|2018-08-10--22-42-26', '9', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-10--22-42-26/9'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '30', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/30'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '33', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/33'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '23', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/23'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '27', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/27'), ('b0c9d2329ad1606b|2018-08-02--16-41-38', '17', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--16-41-38/17'), ('b0c9d2329ad1606b|2018-07-27--06-50-48', '9', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-27--06-50-48/9'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '16', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/16'), ('b0c9d2329ad1606b|2018-08-06--10-04-53', '29', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-06--10-04-53/29'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '39', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/39'), ('b0c9d2329ad1606b|2018-08-17--14-17-47', '12', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-17-47/12'), ('b0c9d2329ad1606b|2018-08-17--14-17-47', '7', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-17-47/7'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '40', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/40'), ('b0c9d2329ad1606b|2018-08-14--20-41-07', '7', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--20-41-07/7'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '14', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/14'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '6', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/6'), ('b0c9d2329ad1606b|2018-08-17--14-55-39', '3', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-55-39/3'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '34', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/34'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '8', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/8'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '33', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/33'), ('b0c9d2329ad1606b|2018-08-06--10-04-53', '30', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-06--10-04-53/30'), ('b0c9d2329ad1606b|2018-08-02--16-41-38', '11', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--16-41-38/11'), ('b0c9d2329ad1606b|2018-08-06--10-04-53', '26', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-06--10-04-53/26'), ('b0c9d2329ad1606b|2018-08-14--20-41-07', '5', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--20-41-07/5'), ('b0c9d2329ad1606b|2018-08-14--20-41-07', '6', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--20-41-07/6'), ('b0c9d2329ad1606b|2018-08-10--22-42-26', '3', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-10--22-42-26/3'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '38', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/38'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '17', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/17'), ('b0c9d2329ad1606b|2018-07-30--13-03-07', '22', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-03-07/22'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '19', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/19'), ('b0c9d2329ad1606b|2018-07-29--12-02-42', '27', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-29--12-02-42/27'), ('b0c9d2329ad1606b|2018-08-02--16-41-38', '14', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--16-41-38/14'), ('b0c9d2329ad1606b|2018-08-14--10-32-01', '31', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-14--10-32-01/31')]\n",
        "val_seqs= [('b0c9d2329ad1606b|2018-08-02--08-34-47', '38', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/38'), ('b0c9d2329ad1606b|2018-08-17--14-55-39', '5', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-55-39/5'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '20', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/20'), ('b0c9d2329ad1606b|2018-07-31--20-50-28', '9', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-31--20-50-28/9'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '22', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/22'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '31', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/31'), ('b0c9d2329ad1606b|2018-08-02--16-41-38', '8', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--16-41-38/8'), ('b0c9d2329ad1606b|2018-08-06--10-04-53', '36', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-06--10-04-53/36'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '15', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/15'), ('b0c9d2329ad1606b|2018-08-02--16-41-38', '10', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--16-41-38/10'), ('b0c9d2329ad1606b|2018-08-17--14-17-47', '10', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-17-47/10'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '10', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/10'), ('b0c9d2329ad1606b|2018-07-27--06-03-57', '11', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-27--06-03-57/11')]\n",
        "test_seqs = [('b0c9d2329ad1606b|2018-08-15--09-01-03', '16', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/16'), ('b0c9d2329ad1606b|2018-08-15--09-01-03', '18', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-15--09-01-03/18'), ('b0c9d2329ad1606b|2018-08-17--14-55-39', '8', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-55-39/8'), ('b0c9d2329ad1606b|2018-08-17--12-07-08', '36', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--12-07-08/36'), ('b0c9d2329ad1606b|2018-07-30--13-44-30', '7', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-44-30/7'), ('b0c9d2329ad1606b|2018-07-30--13-03-07', '16', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-07-30--13-03-07/16'), ('b0c9d2329ad1606b|2018-08-17--14-17-47', '8', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-17-47/8'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '35', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/35'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '29', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/29'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '5', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/5'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '12', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/12'), ('b0c9d2329ad1606b|2018-08-03--10-35-16', '7', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-03--10-35-16/7'), ('b0c9d2329ad1606b|2018-08-17--14-55-39', '4', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-17--14-55-39/4'), ('b0c9d2329ad1606b|2018-08-02--08-34-47', '30', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--08-34-47/30'), ('b0c9d2329ad1606b|2018-08-02--16-41-38', '7', '/content/dataset/Chunk_1/b0c9d2329ad1606b|2018-08-02--16-41-38/7')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Save dataset ---\n",
        "for drive_sequence_path, seq, full_seq_path in tqdm(train_seqs, desc=\"Train\"):\n",
        "    sample = get_sample(full_seq_path)\n",
        "    if sample:\n",
        "        video_id = drive_sequence_path.replace(\"|\", \"_\")\n",
        "        save_h5py(f\"{video_id}_{seq}\", sample, h_train)\n",
        "\n",
        "for drive_sequence_path, seq, full_seq_path in tqdm(val_seqs, desc=\"Val\"):\n",
        "    sample = get_sample(full_seq_path)\n",
        "    if sample:\n",
        "        video_id = drive_sequence_path.replace(\"|\", \"_\")\n",
        "        save_h5py(f\"{video_id}_{seq}\", sample, h_val)\n",
        "\n",
        "for drive_sequence_path, seq, full_seq_path  in tqdm(test_seqs, desc=\"Test\"):\n",
        "    sample = get_sample(full_seq_path)\n",
        "    if sample:\n",
        "        video_id = drive_sequence_path.replace(\"|\", \"_\")\n",
        "        save_h5py(f\"{video_id}_{seq}\", sample, h_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArDGyyJJA-k3"
      },
      "outputs": [],
      "source": [
        "h_test.close()\n",
        "h_train.close()\n",
        "h_val.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cNlbZUpBCo2B",
        "outputId": "51da003a-4d7a-49c8-fd9a-72552acda8c6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6d4e1aed-1124-4e99-94ad-02c750deacd3\", \"gas_and_brake_train_comma_chunk_1_w_imgs.hdf5\", 4266749683)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b08571c5-e141-4cbd-a0bb-07f4988f4765\", \"gas_and_brake_val_comma_chunk_1_w_imgs.hdf5\", 720355403)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d02dbd04-b843-4404-b298-a60da2575c06\", \"gas_and_brake_test_comma_chunk_1_w_imgs.hdf5\", 554402583)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/gas_and_brake_train_comma_chunk_1_w_imgs.hdf5\")\n",
        "files.download(\"/content/gas_and_brake_val_comma_chunk_1_w_imgs.hdf5\")\n",
        "files.download(\"/content/gas_and_brake_test_comma_chunk_1_w_imgs.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell was made by Giuseppe Vacante\n",
        "import os, gc\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "chunk_path = \"/content/dataset/Chunk_1\"   # <- principal folder\n",
        "output_base = \"/content/frames\"           # where to save the images\n",
        "output_size = (1280, 960)                 # size to save the frames\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "last_done = None   # e.g. \"b0c9d2329ad1606b|2018-08-17--14-55-39|9\" or None\n",
        "skip = False if last_done is None else True\n",
        "\n",
        "# ---------- function get_sample ----------\n",
        "def get_sample(p):\n",
        "    \"\"\" Return (sample, reason). sample=None if discarded, reason indicates the reason.\"\"\"\n",
        "    try:\n",
        "        fr = FrameReader(os.path.join(p, 'video.hevc'))\n",
        "        logs = list(LogReader(os.path.join(p, 'raw_log.bz2')))\n",
        "    except Exception as e:\n",
        "        return None, f\"open_failed: {e}\"\n",
        "\n",
        "    try:\n",
        "        # extract fields (sampling 1::5 as in original code)\n",
        "        angle = np.array([l.carState.steeringAngleDeg for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        time = np.array([l.logMonoTime for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        vEgo = np.array([l.carState.vEgo for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        gas = np.array([l.carState.gas for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        gaspressed = np.array([l.carState.gasPressed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        brake = np.array([l.carState.brake for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        brake_pressed = np.array([l.carState.brakePressed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "        leftBlinker = np.array([l.carState.leftBlinker for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        rightBlinker = np.array([l.carState.rightBlinker for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "        enabled = np.array([l.carState.cruiseState.enabled for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        speed = np.array([l.carState.cruiseState.speed for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        speedOffset = np.array([l.carState.cruiseState.speedOffset for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        standstill = np.array([l.carState.cruiseState.standstill for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        nonAdaptive = np.array([l.carState.cruiseState.nonAdaptive for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "        speedCluster = np.array([l.carState.cruiseState.speedCluster for l in logs if l.which() == 'carState'])[1::5][1::5]\n",
        "\n",
        "        # radar distances (similar sampling)\n",
        "        dist = np.array([l.radarState.leadOne.dRel for l in logs if l.which() == \"radarState\"])[1::5]\n",
        "    except Exception as e:\n",
        "        return None, f\"field_extract_failed: {e}\"\n",
        "\n",
        "    # Filters: vEgo/dist too many zeros or dist too short -> discard\n",
        "    if ((vEgo == 0).mean() > 0.2):\n",
        "        return None, f\"vEgo_zeros_frac={(vEgo==0).mean():.3f}\"\n",
        "    if ((dist == 0).mean() > 0.2):\n",
        "        return None, f\"dist_zeros_frac={(dist==0).mean():.3f}\"\n",
        "    if len(dist) <= 230:\n",
        "        return None, f\"dist_too_short={len(dist)}\"\n",
        "\n",
        "    # Read frames (1::5 sampling)\n",
        "    images = []\n",
        "    try:\n",
        "        for idx in range(fr.frame_count)[1::5]:\n",
        "            im = fr.get(idx, pix_fmt='rgb24')[0]  # numpy array H,W,3 RGB\n",
        "            images.append(np.array(im, dtype=np.float32))\n",
        "    except Exception as e:\n",
        "        return None, f\"frame_read_failed: {e}\"\n",
        "\n",
        "    if len(images) == 0:\n",
        "        return None, \"no_images\"\n",
        "\n",
        "    # compute desired_gap as in original code\n",
        "    steady_state = ~gaspressed & ~brake_pressed & ~leftBlinker & ~rightBlinker\n",
        "    desired_gap = np.zeros(steady_state.shape)\n",
        "    last_idx = 0\n",
        "    for i in range(len(steady_state)-1):\n",
        "        if steady_state[i]:\n",
        "            desired_gap[last_idx:i] = int(dist[i])\n",
        "            last_idx = i\n",
        "    desired_gap[-12:] = dist[-12:].mean() if len(dist) >= 12 else (dist.mean() if len(dist)>0 else 0)\n",
        "\n",
        "    if (desired_gap == 0).mean() > 0.2:\n",
        "        return None, f\"desired_gap_zero_frac={(desired_gap==0).mean():.3f}\"\n",
        "\n",
        "    sample = {\n",
        "        'image': images,\n",
        "        'vEgo': vEgo,\n",
        "        'dist': dist,\n",
        "        'desired_dist': desired_gap,\n",
        "    }\n",
        "    return sample, \"ok\"\n",
        "\n",
        "# ---------------- LOOP SUL CHUNK FOLDER ----------------\n",
        "kept_seqs = []               # list of folder_name\n",
        "skipped_seqs = []            # list of tuples (folder_name, reason)\n",
        "n_saved = 0\n",
        "n_skipped = 0\n",
        "n_errors = 0\n",
        "\n",
        "if not os.path.isdir(chunk_path):\n",
        "    raise FileNotFoundError(f\"chunk_path not found: {chunk_path}\")\n",
        "\n",
        "for drive in tqdm(sorted(os.listdir(chunk_path)), desc=\"Processing drives\"):\n",
        "    drive_path = os.path.join(chunk_path, drive)\n",
        "    if not os.path.isdir(drive_path):\n",
        "        continue\n",
        "\n",
        "    for seq in tqdm(sorted(os.listdir(drive_path)), desc=f\"Processing seq in {drive}\", leave=False):\n",
        "        seq_path = os.path.join(drive_path, seq)\n",
        "        video_path = os.path.join(seq_path, \"video.hevc\")\n",
        "        if not os.path.exists(video_path):\n",
        "            skipped_seqs.append((f\"{drive}|{seq}\", \"no_video\"))\n",
        "            n_skipped += 1\n",
        "            continue\n",
        "\n",
        "        folder_name = f\"{drive}|{seq}\"\n",
        "        # manage skip (if you want to restart from last_done)\n",
        "        if skip:\n",
        "            if last_done is not None and folder_name == last_done:\n",
        "                skip = False\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        save_path = os.path.join(output_base, folder_name)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # apply filters with get_sample\n",
        "        try:\n",
        "            sample, reason = get_sample(seq_path)\n",
        "        except Exception as e:\n",
        "            sample = None\n",
        "            reason = f\"exception_in_get_sample: {e}\"\n",
        "\n",
        "        if sample is None:\n",
        "            skipped_seqs.append((folder_name, reason))\n",
        "            n_skipped += 1\n",
        "            continue\n",
        "\n",
        "        # save images in sample['image'], resize and save\n",
        "        try:\n",
        "            images = sample['image']\n",
        "            if len(images) == 0:\n",
        "                skipped_seqs.append((folder_name, \"no_images\"))\n",
        "                n_skipped += 1\n",
        "                continue\n",
        "\n",
        "            saved_idx = 1\n",
        "            for img in images:\n",
        "                arr = np.clip(img, 0, 255).astype(np.uint8)\n",
        "                arr_resized = cv2.resize(arr, output_size, interpolation=cv2.INTER_CUBIC)\n",
        "                arr_bgr = cv2.cvtColor(arr_resized, cv2.COLOR_RGB2BGR)\n",
        "                filename = os.path.join(save_path, f\"{saved_idx:05}.jpg\")\n",
        "                cv2.imwrite(filename, arr_bgr)\n",
        "                saved_idx += 1\n",
        "                n_saved += 1\n",
        "\n",
        "            kept_seqs.append(folder_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            skipped_seqs.append((folder_name, f\"save_error: {e}\"))\n",
        "            n_errors += 1\n",
        "            continue\n",
        "\n",
        "# final report + saving lists\n",
        "print(\"=== Done ===\")\n",
        "print(f\"Saved frames (files): {n_saved}\")\n",
        "print(f\"Skipped sequences (count): {len(skipped_seqs)}\")\n",
        "print(f\"Errors while saving (count): {n_errors}\")\n",
        "print(f\"Kept sequences (count): {len(kept_seqs)}\")\n",
        "\n",
        "# save two text files with the lists (ready for download)\n",
        "kept_file = os.path.join(output_base, \"kept_seqs.txt\")\n",
        "skipped_file = os.path.join(output_base, \"skipped_seqs.txt\")\n",
        "\n",
        "with open(kept_file, \"w\") as f:\n",
        "    for s in kept_seqs:\n",
        "        f.write(s + \"\\n\")\n",
        "\n",
        "with open(skipped_file, \"w\") as f:\n",
        "    for s, reason in skipped_seqs:\n",
        "        f.write(f\"{s}\\t{reason}\\n\")\n",
        "\n",
        "print(f\"Kept list saved to: {kept_file}\")\n",
        "print(f\"Skipped list saved to: {skipped_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "collapsed": true,
        "id": "NO_832N0m4f9",
        "outputId": "cf3b35d3-ddb2-4f1d-fdbb-cf3618b77b66"
      },
      "outputs": [],
      "source": [
        "#Old version - extracts every 5th frame without filtering\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "#from openpilot.tools.lib.video import FrameReader\n",
        "\n",
        "chunk_path = \"/content/dataset/Chunk_1\" \n",
        "output_base = \"/content/frames\"         \n",
        "fps_target = 20\n",
        "output_size = (1280, 960)\n",
        "\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "\n",
        "for drive in tqdm(os.listdir(chunk_path), desc=\"Processing drives\"):\n",
        "    drive_path = os.path.join(chunk_path, drive)\n",
        "    if not os.path.isdir(drive_path): continue\n",
        "\n",
        "    for seq in os.listdir(drive_path):\n",
        "        seq_path = os.path.join(drive_path, seq)\n",
        "        video_path = os.path.join(seq_path, \"video.hevc\")\n",
        "        if not os.path.exists(video_path): continue\n",
        "\n",
        "        folder_name = f\"{drive}|{seq}\"\n",
        "        save_path = os.path.join(output_base, folder_name)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            fr = FrameReader(video_path)\n",
        "            total_frames = fr.frame_count\n",
        "            print(total_frames)\n",
        "            video_fps = 20\n",
        "            step = int(round(video_fps / fps_target)) \n",
        "\n",
        "            frame_idx = 0\n",
        "            saved_idx = 1\n",
        "\n",
        "            for i in range(1, total_frames, 5):\n",
        "                try:\n",
        "                    img = fr.get(i, pix_fmt='rgb24')[0]  # (H, W, 3)\n",
        "                    img = cv2.resize(img, output_size, interpolation=cv2.INTER_CUBIC)\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "                    filename = os.path.join(save_path, f\"{saved_idx:05}.jpg\")\n",
        "                    cv2.imwrite(filename, img)\n",
        "                    saved_idx += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error in frame {i} in {folder_name}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error with {video_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZVJbSy9p4x0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dceknhhpzAIo",
        "outputId": "161d12b8-c030-4e64-dadc-564cafcbdddb"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "folder_path = \"/content/frames\"          \n",
        "output_zip = \"/content/frames.zip\"       \n",
        "shutil.make_archive(\"/content/frames\", 'zip', folder_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
