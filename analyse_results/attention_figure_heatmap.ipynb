{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import pad_collate\n",
    "from dataloader_comma import CommaDataset\n",
    "from collections import Counter\n",
    "from model import VTN\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from utils import * \n",
    "import re\n",
    "from vis_utils import * \n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataloader_comma import CommaDataset\n",
    "\n",
    "# Create the dataset\n",
    "dataset_comma = CommaDataset(\n",
    "    dataset_type=\"test\",  # o \"val\" o \"train\" depends on what we want plot\n",
    "    use_transform=False,\n",
    "    multitask=\"distance\",  \n",
    "    ground_truth=\"desired\", # Change to True to have all the parameters you use in the plot\n",
    "    dataset_path=\"/kaggle/input/final-hdf5-files\",\n",
    "    dataset_fraction=1.0\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset_comma)} samples\")\n",
    "\n",
    "# Creating the Dataloader\n",
    "dataloader_comma = DataLoader(\n",
    "    dataset_comma,\n",
    "    batch_size=1,  # Keep 1 for plotting\n",
    "    shuffle=False,  # Don't shuffle for plots\n",
    "    num_workers=0,  # 0 for easier debugging\n",
    "    # collate_fn=None  # Use the default collate_fn, NOT the custom one you showed\n",
    ")\n",
    "\n",
    "print(\"DataLoader created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f = []\n",
    "for j, batch in enumerate(dataloader_comma):\n",
    "    img, vego, angle, distance, gas, brake, ccenabled, seq_key = batch\n",
    "\n",
    "    intervention = ~ccenabled.squeeze() | brake.squeeze() | gas.squeeze()\n",
    "\n",
    "    img, angle, distance, vego = img.to(gpu), angle.to(gpu), distance.to(gpu), vego.to(gpu)\n",
    "    (logits, attns), concepts = model(img, angle, distance, vego, seq_key)\n",
    "    img, vego, angle, distance, gas, brake, ccenabled = img, vego.squeeze(), angle.squeeze(), distance.squeeze(), gas.squeeze(), brake.squeeze(), ccenabled.squeeze()\n",
    "    angle, distance, vego, logits = angle.to(\"cpu\"), distance.to(\"cpu\"), vego.to(\"cpu\"), logits.detach().cpu().to(\"cpu\")\n",
    "\n",
    "    atten = attns[0][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph_0 = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_0 = moving_average(speed_graph_0, 10)\n",
    "\n",
    "    atten = attns[1][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph_1 = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_1 = moving_average(speed_graph_1, 10)\n",
    "\n",
    "    atten = attns[2][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph_2 = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_2 = moving_average(speed_graph_2, 10)\n",
    "    \n",
    "    f = []\n",
    "    inter = []\n",
    "    m = (speed_graph_2 + speed_graph_1 + speed_graph_0)\n",
    "    plt.plot(speed_graph_2, color=\"black\")\n",
    "    plt.plot(speed_graph_1, color =\"blue\")\n",
    "    plt.plot(speed_graph_0, color =\"red\")\n",
    "    ali = 1 + m \n",
    "    concepts = concepts.squeeze()\n",
    "    weighted_concepts = concepts.cpu() * ali.reshape((ali.shape[0],1))\n",
    "   \n",
    "    smoothed_signal = np.convolve(m, np.ones(8) / 8, mode='same')\n",
    "    peaks, _ = find_peaks(smoothed_signal, distance=16)\n",
    "\n",
    "    # Plot original and smoothed signals\n",
    "    plt.plot(peaks, smoothed_signal[peaks], 'ro', label='Peaks')\n",
    "    plt.plot(smoothed_signal, 'r-')\n",
    "    binary_array = np.zeros(len(m), dtype=np.int64)\n",
    "    \n",
    "\n",
    "\n",
    "    for elem in peaks:\n",
    "        binary_array[max(0, elem-4): min(len(m), elem+4)] = 1\n",
    "    binary_array = (smoothed_signal-smoothed_signal.mean()) < 0.1\n",
    "    attention_concepts = concepts.squeeze()[binary_array == 1, :]\n",
    "    top5_indices = torch.tensor(attention_concepts).topk(5).indices\n",
    "    for i, elem0 in enumerate(top5_indices):\n",
    "        for a in elem0: \n",
    "            f.append(scenarios)\n",
    "    all_f.extend(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the top N concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = []\n",
    "for f in all_f: \n",
    "    flattened.extend(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.unique(np.array(flattened))\n",
    "c = Counter(np.array(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = c.most_common(6)\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = c.most_common(6)\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, batch in enumerate(dataloader_comma):\n",
    "    img, vego, angle, distance, gas, brake, ccenabled, seq_key = batch\n",
    "\n",
    "    img, angle, distance, vego = img.to(gpu), angle.to(gpu), distance.to(gpu), vego.to(gpu)\n",
    "    (logits, attns), concepts = model(img, angle, distance, vego, seq_key)\n",
    "    img, vego, angle, distance, gas, brake, ccenabled = img, vego.squeeze(), angle.squeeze(), distance.squeeze(), gas.squeeze(), brake.squeeze(), ccenabled.squeeze()\n",
    "    logits, attns = logits.squeeze(), attns.squeeze()\n",
    "    attns = attns[ccenabled]\n",
    "    angle, distance, vego, logits = angle.to(\"cpu\"), distance.to(\"cpu\"), vego.to(\"cpu\"), logits.detach().cpu().to(\"cpu\")\n",
    "\n",
    "    concepts\n",
    "    atten = attns[0][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph_0 = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_0 = moving_average(speed_graph_0, 10)\n",
    "\n",
    "    atten = attns[1][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph_1 = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_1 = moving_average(speed_graph_1, 10)\n",
    "\n",
    "    atten = attns[2][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph_2 = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_2 = moving_average(speed_graph_2, 10)\n",
    "    \n",
    "    f = []\n",
    "    inter = []\n",
    "    m = (speed_graph_2 + speed_graph_1 + speed_graph_0)/3\n",
    "    ali = 1 + m \n",
    "    print(ali.shape)\n",
    "    concepts = concepts.squeeze()\n",
    "    weighted_concepts = concepts * ali\n",
    "\n",
    "    top5_indices = torch.tensor(concepts).topk(5).indices\n",
    "    \n",
    "    for i, elem0 in enumerate(top5_indices):\n",
    "        inter = []\n",
    "        for elem in top5_indices[max(i-5, 0):min(i+5,len(top5_indices))]:\n",
    "            l = elem.cpu().numpy().tolist()\n",
    "            if 131 in l:\n",
    "                l.remove(131)\n",
    "            inter.extend(l)\n",
    "        count_dict = Counter(inter)\n",
    "        # Get the top 5 most occurring numbers\n",
    "        top_5 = count_dict.most_common(2)\n",
    "        intermediate = []\n",
    "        for a in top_5: \n",
    "            intermediate.append(scenarios[a[0]])\n",
    "        f.append(intermediate)\n",
    "\n",
    "    normalized_weights = atten#np.array(attention_weights) / np.sum(attention_weights)\n",
    "    print(atten.shape)\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.plot(speed_graph_0, label=\"1\")\n",
    "    plt.plot(speed_graph_1, label=\"2\")\n",
    "    plt.plot(speed_graph_2, label=\"3\")\n",
    "    plt.plot(m, label=\"mean\")\n",
    "    plt.plot(alignment_array[0:70, :].sum(axis=0), label=\"align\")\n",
    "    plt.legend()\n",
    "    #heatmap = ax.pcolormesh(alignment_array[0:70, :], cmap='hot')\n",
    "    # Add colorbar\n",
    "    #cbar = plt.colorbar(heatmap)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Sequence Position')\n",
    "    ax.set_ylabel('Window')\n",
    "    ax.set_title('Longformer Sliding Chunk Attention')\n",
    "    plt.show()\n",
    "    # Show the plot\n",
    "    #plt.savefig(f\"/kaggle/working/attention_vis{j}.pdf\")\n",
    "    #plt.close()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 26}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
