{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using concept features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from vis_utils import * \n",
    "sys.path.append(\"..\")\n",
    "from utils import pad_collate\n",
    "from dataloader_comma import CommaDataset\n",
    "from collections import Counter\n",
    "from model import VTN\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from utils import * \n",
    "import re\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataloader_comma import CommaDataset\n",
    "\n",
    "# Create the dataset\n",
    "dataset_comma = CommaDataset(\n",
    "    dataset_type=\"test\",  # o \"val\" o \"train\" depends on what we want plot\n",
    "    use_transform=False,\n",
    "    multitask=\"distance\",  \n",
    "    ground_truth=\"desired\", # Change to True to have all the parameters you use in the plot\n",
    "    dataset_path=\"/kaggle/input/final-hdf5-files\",\n",
    "    dataset_fraction=1.0\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset_comma)} samples\")\n",
    "\n",
    "# Creating the Dataloader\n",
    "dataloader_comma = DataLoader(\n",
    "    dataset_comma,\n",
    "    batch_size=1,  # Keep 1 for plotting\n",
    "    shuffle=False,  # Don't shuffle for plots\n",
    "    num_workers=0,  # 0 for easier debugging\n",
    "    # collate_fn=None  # Use the default collate_fn, NOT the custom one you showed\n",
    ")\n",
    "\n",
    "print(\"DataLoader created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2176445/171860473.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  top5_indices = torch.tensor(concepts.squeeze()).topk(5).indices\n"
     ]
    }
   ],
   "source": [
    "concs = []\n",
    "gt = []\n",
    "## Changed the dataloader \n",
    "for j, batch in enumerate(dataloader_comma):\n",
    "    image_array,  vego, angle, distance, g, s, l, seq_key = batch #g= gaspressed, s= brakepressed, l= cruiseenabled\n",
    "\n",
    "    #image_array, distance, angle, vego, seq_key = batch\n",
    "    img = image_array\n",
    "    img, angle, distance, vego = img.to(f'cuda:{gpu_num}'), angle.to(f'cuda:{gpu_num}'), distance.to(f'cuda:{gpu_num}'), vego.to(f'cuda:{gpu_num}')\n",
    "    (logits, attns), concepts = model(img, angle, distance, vego, seq_key)\n",
    "    top5_indices = torch.tensor(concepts.squeeze()).topk(5).indices\n",
    "    s = img.shape#[batch_size, seq_len, h,w,c]\n",
    "    angle, distance, vego = angle.to(\"cpu\"), distance.to(\"cpu\"), vego.to(\"cpu\")\n",
    "    logits = logits.detach().cpu().to(\"cpu\")\n",
    "    concepts = concepts.detach().cpu().to(\"cpu\")\n",
    "\n",
    "    att = attns[0][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = att.shape[2]\n",
    "    atten = att\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_0 = moving_average(speed_graph, 5)\n",
    "\n",
    "    att = attns[1][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = att.shape[2]\n",
    "    atten = att\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_1 = moving_average(speed_graph, 5)\n",
    "    concs.extend(top5_indices.squeeze().cpu().tolist())\n",
    "    gt.extend(distance.squeeze().cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       large\n",
       "0       large\n",
       "0       large\n",
       "0       large\n",
       "0       large\n",
       "        ...  \n",
       "8003    large\n",
       "8003    large\n",
       "8003    large\n",
       "8003    large\n",
       "8003    large\n",
       "Name: gt_class, Length: 40020, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded['gt_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "filt = lambda x: \"<10\" if x < 10 else (\">10, <30\" if x < 30 and x > 10 else (\">30, <50\") if x < 50 and x > 30 else \">50\")\n",
    "filt = lambda x: \"small\" if x < 5  and x > -5 else 'large'\n",
    "df['gt']  = gt\n",
    "df['concepts'] = concs\n",
    "df_exploded = df.explode('concepts')\n",
    "df_exploded['gt_class'] = df_exploded[\"gt\"].apply( filt)\n",
    "grouped = df_exploded.groupby(by=\"gt_class\")['concepts'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scens = []\n",
    "for i in range(len(grouped)): \n",
    "    count_dict = Counter(grouped.iloc[i])\n",
    "    top_5 = count_dict.most_common(10)\n",
    "    scens = []\n",
    "    for elem in top_5:\n",
    "        if elem[0] == 131: continue\n",
    "        scens.append((scenarios[elem[0]], elem[1]))\n",
    "    all_scens.append(scens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_df = pd.DataFrame(grouped)\n",
    "conc_df['all_scens'] = all_scens\n",
    "conc_df['len'] = conc_df['concepts'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concepts</th>\n",
       "      <th>all_scens</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>[568, 491, 538, 541, 368, 568, 491, 632, 299, ...</td>\n",
       "      <td>[(a photo of a street while overtaking at t ju...</td>\n",
       "      <td>20255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small</th>\n",
       "      <td>[568, 627, 632, 299, 634, 568, 627, 368, 491, ...</td>\n",
       "      <td>[(a photo of a street with a road blocked rela...</td>\n",
       "      <td>19765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   concepts  \\\n",
       "gt_class                                                      \n",
       "large     [568, 491, 538, 541, 368, 568, 491, 632, 299, ...   \n",
       "small     [568, 627, 632, 299, 634, 568, 627, 368, 491, ...   \n",
       "\n",
       "                                                  all_scens    len  \n",
       "gt_class                                                            \n",
       "large     [(a photo of a street while overtaking at t ju...  20255  \n",
       "small     [(a photo of a street with a road blocked rela...  19765  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    'rgba(228, 122, 122, 0.5)',  # Light Red\n",
    "    'rgba(249, 173, 144, 0.5)',  # Light Orange\n",
    "    'rgba(252, 216, 144, 0.5)',  # Light Yellow\n",
    "    'rgba(211, 229, 157, 0.5)',  # Light Green\n",
    "    'rgba(161, 218, 180, 0.5)',  # Light Teal\n",
    "    'rgba(138, 202, 235, 0.5)',  # Light Blue\n",
    "    'rgba(164, 156, 207, 0.5)',  # Light Purple\n",
    "    'rgba(212, 161, 199, 0.5)',  # Light Pink\n",
    "    'rgba(223, 223, 223, 0.5)',  # Light Gray\n",
    "    'rgba(180, 180, 180, 0.5)'   # Light Dark Gray\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conc_df.iloc[-1].all_scens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    {'label': '<b>Distance <10</b>', 'color': \"white\"}\n",
    "]\n",
    "\n",
    "links = []\n",
    "leng = conc_df.iloc[0].len\n",
    "\n",
    "for i, elem in enumerate(conc_df.iloc[0].all_scens):\n",
    "    #expand_label used for retinanet\n",
    "    #label = \"<b>\" +  elem[0].replace(\"a photo of\", \"\").replace(\"driving on\", \"\").replace(\"a highway with\", \"\").replace(\"a street while\", \"\").replace(\"a street with\", \"\") + \"</b>\"\n",
    "    label = \"<b>\" + expand_label(elem[0]) + \"</b>\"\n",
    "\n",
    "    nodes.append({'label':label, 'color': colors[i]})\n",
    "    res = {'source': 0, 'target': i+1, 'value': elem[1]/leng, 'color': colors[i]}\n",
    "    links.append(res)\n",
    "\n",
    "# Create the Sankey diagram figure\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        label=[node['label'] for node in nodes],\n",
    "        color=[node['color'] for node in nodes]\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=[link['source'] for link in links],\n",
    "        target=[link['target'] for link in links],\n",
    "        value=[link['value'] for link in links],\n",
    "        color=[link['color'] for link in links]\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Customize the layout of the Sankey diagram\n",
    "fig.update_layout(\n",
    "    title_text='',\n",
    "    width=1200,  # Set the width of the plot\n",
    "    height=500,  # Set the height of the plot\n",
    "    font=dict(size=20, color='black'),\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=0,\n",
    "        r=0,\n",
    "        t=0,\n",
    "        b=0\n",
    "    ),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Display the Sankey diagram\n",
    "fig.show()\n",
    "pio.write_image(fig, 'dist10.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
