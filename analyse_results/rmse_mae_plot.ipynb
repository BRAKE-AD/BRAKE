{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import pad_collate\n",
    "from dataloader_comma import CommaDataset\n",
    "from model import VTN\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from utils import * \n",
    "import re\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(input, target, mask, reduction=\"mean\"):\n",
    "        out = (input[~mask]-target[~mask])**2\n",
    "        return out.mean().sqrt() if reduction == \"mean\" else out \n",
    "\n",
    "def mae_loss(input, target, mask, reduction=\"mean\"):\n",
    "        out = (input[~mask]-target[~mask])\n",
    "        return out.mean().abs() if reduction == \"mean\" else out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argparse_namespace_constructor(loader, node):\n",
    "    values = loader.construct_mapping(node)\n",
    "    namespace = argparse.Namespace()\n",
    "    namespace.__dict__.update(values)\n",
    "    return namespace\n",
    "\n",
    "# Register the custom constructor with PyYAML\n",
    "yaml.add_constructor('tag:yaml.org,2002:python/object:argparse.Namespace', argparse_namespace_constructor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_comma import CommaDataset\n",
    "\n",
    "\n",
    "def get_regular_ckpt_from_lightning_checkpoint(state_dict):\n",
    "    try:\n",
    "        new_sd = {}\n",
    "        for k, v in state_dict.items():\n",
    "            nk = k.replace(\"model.\", \"\") if k.startswith(\"model.\") else k\n",
    "            new_sd[nk] = v\n",
    "        return new_sd\n",
    "    except Exception:\n",
    "        return state_dict\n",
    "\n",
    "def find_latest_checkpoint_and_yaml(root_dir: str):\n",
    "    root = Path(root_dir)\n",
    "    ckpts = list(root.rglob(\"*.ckpt\"))\n",
    "    if not ckpts:\n",
    "        raise FileNotFoundError(f\"No .ckpt files found under {root_dir}\")\n",
    "    # last modified\n",
    "    latest_ckpt = max(ckpts, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "    # try to find a yaml in the same folder (or parent)\n",
    "    yaml_file = None\n",
    "    candidate_dirs = [latest_ckpt.parent, latest_ckpt.parent.parent]\n",
    "    for d in candidate_dirs:\n",
    "        if d is None: continue\n",
    "        for ext in (\"*.yaml\", \"*.yml\"):\n",
    "            ys = list(d.glob(ext))\n",
    "            if ys:\n",
    "                # pick latest yaml by mtime\n",
    "                yaml_file = max(ys, key=lambda p: p.stat().st_mtime)\n",
    "                break\n",
    "        if yaml_file:\n",
    "            break\n",
    "\n",
    "    yaml_data = None\n",
    "    if yaml_file:\n",
    "        with yaml_file.open(\"r\") as f:\n",
    "            try:\n",
    "                yaml_data = yaml.safe_load(f)\n",
    "            except Exception:\n",
    "                yaml_data = None\n",
    "\n",
    "    return str(latest_ckpt), (str(yaml_file) if yaml_file else None), yaml_data\n",
    "\n",
    "# Example:\n",
    "root = \"/kaggle/input/comma-ckpt-seed123/ckpts_final_comma_distance_none_True_1_clip\"\n",
    "ckpt_path, yaml_path, yaml_data = find_latest_checkpoint_and_yaml(root)\n",
    "print(\"Checkpoint:\", ckpt_path)\n",
    "print(\"Yaml:\", yaml_path)\n",
    "# print(\"Yaml data:\", yaml_data)\n",
    "\n",
    "\n",
    "dataset_comma = CommaDataset(\n",
    "    dataset_type=\"test\", \n",
    "    use_transform=False,\n",
    "    multitask=\"distance\",\n",
    "    ground_truth=\"desired\",\n",
    "    return_full=True,  \n",
    "    dataset_path=\"/kaggle/input/final-hdf5-files\",\n",
    "    dataset_fraction=1.0\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset_comma)} samples\")\n",
    "\n",
    "dataloader_comma = DataLoader(\n",
    "    dataset_comma,\n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "   \n",
    ")\n",
    "\n",
    "model = VTN(multitask='distance', backbone='none', concept_features= True, device = 'cuda:1', return_concepts=True, concept_source = 'clip')\n",
    "\n",
    "\n",
    "# Safe loading of the checkpoint (map to CPU if necessary)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "ckpt = torch.load(checkpoint_path)\n",
    "state_dict = ckpt['state_dict']\n",
    "state_dict = get_regular_ckpt_from_lightning_checkpoint(state_dict)\n",
    "# if you want to avoid mismatch\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(state_dict)\n",
    "except Exception as e:\n",
    "    print(\"Load state_dict strict failed, retrying with strict=False:\", e)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems = []\n",
    "n_feats = []\n",
    "p = '/kaggle/input/comma-ckpt-seed123/'\n",
    "\n",
    "for elem in os.listdir(p):\n",
    "    l = p + elem + \"/lightning_logs\"\n",
    "    if len(os.listdir(p + elem)) == 0: continue\n",
    "    for version in os.listdir(l):\n",
    "        k = f'{l}/{version}/checkpoints/'\n",
    "        if 'checkpoints' not in os.listdir(f'{l}/{version}'): continue \n",
    "        for filename in os.listdir(k):\n",
    "            if \"yaml\" in filename:\n",
    "                with open(k + filename, \"r\") as file:\n",
    "                    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "                    data_dict = vars(yaml_data)\n",
    "                if 'scenario_type' in data_dict:\n",
    "                    print(k + filename)\n",
    "                    elems.append(k)\n",
    "                    n_feats.append(data_dict['scenario_type'])\n",
    "\n",
    "res = {}\n",
    "for i, elem in enumerate(elems):\n",
    "    task = []\n",
    "    for filename in os.listdir(elem):\n",
    "        if '.csv' in filename: \n",
    "            df = pd.read_csv(elem + filename)\n",
    "            df.columns = ['preds', 'targets']\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'] > 70).astype(bool)  if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            \n",
    "            \n",
    "            loss3 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            task.append(round(loss3.item(), 2))\n",
    "            res[filename + n_feats[i]] = task\n",
    "\n",
    "rows = []\n",
    "for elem in res.keys():\n",
    "    task = \"dist\" if \"dist\" in elem else (\"angle\" if 'angle' in elem else \"multitask\")\n",
    "    dataset = \"comma\"\n",
    "    size = 100 if '100' in elem else (300 if '300' in elem else (24 if '24' in elem else 48 if '48' in elem else \"none\"))\n",
    "    val = res[elem]\n",
    "    if task == 'multitask' or size == \"none\": continue\n",
    "    rows.append({'task': task, 'dataset': dataset, \"MAE\": val[0], \"size\":size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/kaggle/input/comma-ckpt-seed123/'\n",
    "\n",
    "experiments = os.listdir(p)\n",
    "res = {}\n",
    "res_abl = {}\n",
    "res_ckpt = {}\n",
    "for elem in experiments:\n",
    "    if 'ablation' in elem or \"0.75\" in elem or '0.25' in elem or \"1\" in elem or '0.5' in elem: continue\n",
    "    path = p + elem + \"/lightning_logs/\" \n",
    "    if len(os.listdir(p + elem)) == 0: continue\n",
    "    vs = os.listdir(path)\n",
    "    filt = []\n",
    "    for elem1 in vs: \n",
    "        if 'version' in elem1:\n",
    "            filt.append(elem1)\n",
    "    versions =[int(elem.split(\"_\")[-1])for elem in filt]\n",
    "    versions = sorted(versions)\n",
    "    if len(versions) == 0: continue\n",
    "    version = f\"version_{versions[-1]}\"\n",
    "    #print(versions, version)\n",
    "    \n",
    "\n",
    "    checkpoint_path = path + version + \"/checkpoints/\"\n",
    "    if \"checkpoints\" not in os.listdir(path + version): continue\n",
    "    files = os.listdir(checkpoint_path)\n",
    "\n",
    "    task = []\n",
    "    ckpt = []\n",
    "    task_abl = []\n",
    "    files = sorted(files)\n",
    "\n",
    "    if \"hparams.yaml\" in files: \n",
    "        with open(checkpoint_path + \"/hparams.yaml\" , \"r\") as file:   \n",
    "            yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "            data_dict = vars(yaml_data)\n",
    "            if 'dataset_fraction' in data_dict:\n",
    "                if  data_dict['dataset_fraction'] != 1:\n",
    "                    print(elem)\n",
    "                    continue\n",
    "            if 'n_scenarios' in data_dict:\n",
    "                if  data_dict['n_scenarios'] != 1 :\n",
    "                    print(elem, data_dict['n_scenarios'])\n",
    "                    continue\n",
    "    for filename in files: \n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(checkpoint_path + filename)\n",
    "            df.columns = ['preds', 'targets']\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'] > 70).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            \n",
    "            loss3 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            loss4 = rmse_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            if \"00\" in filename:\n",
    "                task_abl.append(round(loss3.item(), 2))\n",
    "            else:\n",
    "                task.append((round(loss3.item(), 2),round(loss4.item(), 2)))\n",
    "        if filename.endswith(\".ckpt\"):\n",
    "            ckpt.append(checkpoint_path + '/' + filename)\n",
    "    res[elem] = task\n",
    "    if len(task_abl) != 0:\n",
    "        res_abl[elem] = task_abl\n",
    "    res_ckpt[elem] = ckpt\n",
    "rows = []\n",
    "for elem in res.keys():\n",
    "    splitted = elem.split(\"_\")\n",
    "    data = splitted[2]\n",
    "    task = splitted[3]\n",
    "    backbone = splitted[4]\n",
    "    if len(splitted) > 5:\n",
    "        concept = splitted[5]\n",
    "    else: \n",
    "        concept = False\n",
    "    rows.append({\"Concept\": concept, \"Task\": task, \"Backbone\": backbone, \"Dataset\": data, \"t-RMSE\": res[elem]})\n",
    "\n",
    "#pd.DataFrame(rows).sort_values('size').sort_values('task')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of dictionaries\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = pd.pivot_table(df, values='t-RMSE', index=['Dataset', 'Concept', 'Concept'], columns='Task', aggfunc=lambda x: x)\n",
    "\n",
    "# Rename the columns\n",
    "df_pivot.columns = [f'{col}-MAE' for col in df_pivot.columns]\n",
    "# Reset the index\n",
    "df_pivot = df_pivot.reset_index()\n",
    "#df_pivot['Feat. Size'] = [512, , 512, 768, 512, , 512, 768]\n",
    "#df_pivot = df_pivot[[\"Dataset\", \"Concept\", 'Feat. Size', 'angle-MAE', 'distance-MAE', 'multiangle-MAE', 'multidistance-MAE']].round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_pivot[[\"Dataset\", \"Concept\", 'angle-MAE', 'distance-MAE', 'multitask-MAE']].round(2)\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = '/kaggle/working/preds/'\n",
    "\n",
    "experiments = os.listdir(p)\n",
    "res = {}\n",
    "res_ckpt = {}\n",
    "for filename in experiments: \n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(p + filename)\n",
    "        df.columns = ['preds', 'targets']\n",
    "        m = (df['targets'] == 0).astype(bool) | (df['targets'] > 40).astype(bool)  if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "        \n",
    "        loss3 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "        res[filename] = round(loss3.item(), 2)\n",
    "rows = []\n",
    "for elem in res.keys():\n",
    "    splitted = elem.split(\"_\")\n",
    "    print(elem)\n",
    "    task = \"multi\" if 'multi' in elem else (\"angle\" if 'angle' in elem else 'distance')\n",
    "    if \"multi\" in task:\n",
    "        task = task + (\"angle\" if \"angle\" in elem else \"distance\")\n",
    "    backbone = \"clip\"  \n",
    "    data = 'comma'\n",
    "    rows.append({\"Task\": task, \"Concept\": backbone, \"Dataset\": data, \"t-RMSE\": round(res[elem], 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of dictionaries\n",
    "df = pd.DataFrame(rows)\n",
    "data = rows\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = pd.pivot_table(df, values='t-RMSE', index=['Dataset', 'Concept'], columns='Task', aggfunc=lambda x: x)\n",
    "\n",
    "# Rename the columns\n",
    "df_pivot.columns = [f'{col}-MAE' for col in df_pivot.columns]\n",
    "# Reset the index   \n",
    "df_pivot = df_pivot.reset_index()\n",
    "#df_pivot['Feat. Size'] = [512, , 512, 768, 512, , 512, 768]\n",
    "#df_pivot = df_pivot[[\"Dataset\", \"Concept\", 'Feat. Size', 'angle-MAE', 'distance-MAE', 'multiangle-MAE', 'multidistance-MAE']].round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pivot = df_pivot[[\"Dataset\", \"Concept\", 'angle-MAE', 'distance-MAE', 'multiangle-MAE', 'multidistance-MAE']].round(2)\n",
    "df_pivot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/kaggle/input/comma-ckpt-seed123/'\n",
    "\n",
    "experiments = os.listdir(p)\n",
    "res = {}\n",
    "res_abl = {}\n",
    "res_ckpt = {}\n",
    "for elem in experiments:\n",
    "    path = p + elem + \"/lightning_logs/\" \n",
    "    if len(os.listdir(p + elem)) == 0: continue\n",
    "    vs = os.listdir(path)\n",
    "    filt = []\n",
    "    for elem1 in vs: \n",
    "        if 'version' in elem1:\n",
    "            filt.append(elem1)\n",
    "    versions =[int(elem.split(\"_\")[-1])for elem in filt]\n",
    "    versions = sorted(versions)\n",
    "    if len(versions) == 0: continue\n",
    "    version = f\"version_{versions[-1]}\"\n",
    "    \n",
    "    checkpoint_path = path + version + \"/checkpoints/\"\n",
    "    if \"checkpoints\" not in os.listdir(path + version): continue\n",
    "    files = os.listdir(checkpoint_path)\n",
    "\n",
    "    task = []\n",
    "    ckpt = []\n",
    "    task_abl = []\n",
    "    files = sorted(files)\n",
    "    if sum([\".csv\" in filename for filename in files]) == 0: continue\n",
    "    print(files)\n",
    "    for filename in files: \n",
    "        if filename.endswith(\".ckpt\"):\n",
    "            ckpt.append(checkpoint_path + '/' + filename)\n",
    "    res_ckpt[elem] = ckpt\n",
    "\n",
    "rows = []\n",
    "\n",
    "for elem in res_ckpt.keys():\n",
    "    minint = 1000000000\n",
    "    for ckpt in res_ckpt[elem]:\n",
    "        ckpt_num = int(ckpt.split('epoch=')[1].split(\"-step=\")[0])\n",
    "        if minint > ckpt_num: \n",
    "            minint = ckpt_num\n",
    "        \n",
    "\n",
    "    splitted = elem.split(\"_\")\n",
    "    data = splitted[2]\n",
    "    task = splitted[3]\n",
    "    backbone = splitted[4]\n",
    "    if len(splitted) > 5:\n",
    "        concept = splitted[5]\n",
    "    else: \n",
    "        concept = backbone == \"none\"\n",
    "    rows.append({\"Concept\": concept, \"Task\": task, \"Concept_source\": backbone, \"Dataset\": data, \"ckpt\": minint})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows).groupby(by='Backbone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt = df.sort_values(\"Task\").sort_values(\"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/kaggle/input/comma-ckpt-seed123'\n",
    "\n",
    "experiments = os.listdir(p)\n",
    "res = {}\n",
    "res_abl = {}\n",
    "res_ckpt = {}\n",
    "for elem in experiments:\n",
    "    if 'ablation' in elem or \"0.75\" in elem or '0.25' in elem or \"1\" in elem or '0.5' in elem: continue\n",
    "    path = p + elem + \"/lightning_logs/\" \n",
    "    if len(os.listdir(p + elem)) == 0: continue\n",
    "    vs = os.listdir(path)\n",
    "    filt = []\n",
    "    for elem1 in vs: \n",
    "        if 'version' in elem1:\n",
    "            filt.append(elem1)\n",
    "    versions =[int(elem.split(\"_\")[-1])for elem in filt]\n",
    "    versions = sorted(versions)\n",
    "    if len(versions) == 0: continue\n",
    "    version = f\"version_{versions[-1]}\"\n",
    "    #print(versions, version)    \n",
    "\n",
    "    checkpoint_path = path + version + \"/checkpoints/\"\n",
    "    if \"checkpoints\" not in os.listdir(path + version): continue\n",
    "    files = os.listdir(checkpoint_path)\n",
    "\n",
    "    task = []\n",
    "    ckpt = []\n",
    "    task_abl = []\n",
    "    files = sorted(files)\n",
    "\n",
    "    if \"hparams.yaml\" in files: \n",
    "        with open(checkpoint_path + \"/hparams.yaml\" , \"r\") as file:   \n",
    "            yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "            data_dict = vars(yaml_data)\n",
    "            if 'dataset_fraction' in data_dict:\n",
    "                if  data_dict['dataset_fraction'] != 1:\n",
    "                    print(elem)\n",
    "                    continue\n",
    "            if 'n_scenarios' in data_dict:\n",
    "                if  data_dict['n_scenarios'] != 1:\n",
    "                    print(elem, data_dict['n_scenarios'])\n",
    "                    continue\n",
    "    for filename in files: \n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(checkpoint_path + filename)\n",
    "            df.columns = ['preds', 'targets']\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 20).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            \n",
    "            loss3 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 40).astype(bool) | (df['targets'].abs() <= 20).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            loss4 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 60).astype(bool) | (df['targets'].abs() <= 40).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            loss5 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() < 60).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            loss7 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            if \"00\" in filename:\n",
    "                task_abl.append(round(loss3.item(), 2))\n",
    "            else:\n",
    "                task.append((round(loss3.item(), 2),round(loss4.item(), 2),round(loss5.item(), 2),round(loss7.item(), 2)))\n",
    "        if filename.endswith(\".ckpt\"):\n",
    "            ckpt.append(checkpoint_path + '/' + filename)\n",
    "    res[elem] = task\n",
    "    if len(task_abl) != 0:\n",
    "        res_abl[elem] = task_abl\n",
    "    res_ckpt[elem] = ckpt\n",
    "rows = []\n",
    "for elem in res.keys():\n",
    "    splitted = elem.split(\"_\")\n",
    "    data = splitted[2]\n",
    "    task = splitted[3]\n",
    "    backbone = splitted[4]\n",
    "    if len(splitted) > 5:\n",
    "        concept = splitted[5]\n",
    "    else: \n",
    "        concept = False\n",
    "    rows.append({\"Concept\": concept, \"Task\": task, \"Concept_source\": backbone, \"Dataset\": data, \"t-RMSE\": res[elem]})\n",
    "\n",
    "#pd.DataFrame(rows).sort_values('size').sort_values('task')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of dictionaries\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = pd.pivot_table(df, values='t-RMSE', index=['Dataset', 'Concept_source', 'Concept'], columns='Task', aggfunc=lambda x: x)\n",
    "\n",
    "# Rename the columns\n",
    "df_pivot.columns = [f'{col}-MAE' for col in df_pivot.columns]\n",
    "# Reset the index\n",
    "df_pivot = df_pivot.reset_index()\n",
    "#df_pivot['Feat. Size'] = [512, , 512, 768, 512, , 512, 768]\n",
    "#df_pivot = df_pivot[[\"Dataset\", \"Backbone\", 'Feat. Size', 'angle-MAE', 'distance-MAE', 'multiangle-MAE', 'multidistance-MAE']].round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_comma = df_pivot[df_pivot['Dataset'] =='comma']\n",
    "categories = [\"[0,19]\", \"[20,40]\", \"[41,60]\", \"[60,]\"]\n",
    "for i in range(df_pivot_comma['distance-MAE'].shape[0]):\n",
    "    angle = list(pd.Series(df_pivot_comma['distance-MAE'].iloc[i][0]).interpolate(method='linear'))\n",
    "    x_positions = np.arange(len(categories))\n",
    "    plt.plot(x_positions, list(angle))\n",
    "\n",
    "# Relabel the x-axis tick positions with the original categorical values\n",
    "plt.xticks(x_positions, categories)\n",
    "plt.title(\"Comma2k19\",fontsize=16)\n",
    "plt.legend(['CLIP', \"Retinanet\"], fontsize=13)\n",
    "plt.xlabel('Distance', fontsize=16)\n",
    "\n",
    "plt.ylabel('MAE', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.savefig(\"dist_comma.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/data1/jessica/data/toyota/ckpts_final/'\n",
    "\n",
    "experiments = os.listdir(p)\n",
    "res = {}\n",
    "res_abl = {}\n",
    "res_ckpt = {}\n",
    "for elem in experiments:\n",
    "    if 'ablation' in elem or \"0.75\" in elem or '0.25' in elem or \"1\" in elem or '0.5' in elem: continue\n",
    "    path = p + elem + \"/lightning_logs/\" \n",
    "    if len(os.listdir(p + elem)) == 0: continue\n",
    "    vs = os.listdir(path)\n",
    "    filt = []\n",
    "    for elem1 in vs: \n",
    "        if 'version' in elem1:\n",
    "            filt.append(elem1)\n",
    "    versions =[int(elem.split(\"_\")[-1])for elem in filt]\n",
    "    versions = sorted(versions)\n",
    "    if len(versions) == 0: continue\n",
    "    version = f\"version_{versions[-1]}\"\n",
    "   \n",
    "    checkpoint_path = path + version + \"/checkpoints/\"\n",
    "    if \"checkpoints\" not in os.listdir(path + version): continue\n",
    "    files = os.listdir(checkpoint_path)\n",
    "\n",
    "    task = []\n",
    "    ckpt = []\n",
    "    task_abl = []\n",
    "    files = sorted(files)\n",
    "\n",
    "    if \"hparams.yaml\" in files: \n",
    "        with open(checkpoint_path + \"/hparams.yaml\" , \"r\") as file:   \n",
    "            yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "            data_dict = vars(yaml_data)\n",
    "            if 'dataset_fraction' in data_dict:\n",
    "                if  data_dict['dataset_fraction'] != 1:\n",
    "                    print(elem)\n",
    "                    continue\n",
    "            if 'n_scenarios' in data_dict:\n",
    "                if  data_dict['n_scenarios'] != 1:\n",
    "                    print(elem, data_dict['n_scenarios'])\n",
    "                    continue\n",
    "    for filename in files: \n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(checkpoint_path + filename)\n",
    "            df.columns = ['preds', 'targets']\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 10).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            \n",
    "            loss3 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 20).astype(bool) | (df['targets'].abs() <= 10).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            loss4 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 30).astype(bool) | (df['targets'].abs() <= 20).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            loss5 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            m = (df['targets'] == 0).astype(bool) | (df['targets'].abs() > 40).astype(bool) | (df['targets'].abs() <= 30).astype(bool)  #if \"angle\" not in elem  else (df['targets'] == np.inf).astype(bool)\n",
    "            loss6 = mae_loss(torch.tensor(df['preds']),  torch.tensor(df['targets']), torch.tensor(m))\n",
    "            if \"00\" in filename:\n",
    "                task_abl.append(round(loss3.item(), 2))\n",
    "            else:\n",
    "                task.append((round(loss3.item(), 2),round(loss4.item(), 2),round(loss5.item(), 2),round(loss5.item(), 2)))\n",
    "        if filename.endswith(\".ckpt\"):\n",
    "            ckpt.append(checkpoint_path + '/' + filename)\n",
    "    res[elem] = task\n",
    "    if len(task_abl) != 0:\n",
    "        res_abl[elem] = task_abl\n",
    "    res_ckpt[elem] = ckpt\n",
    "rows = []\n",
    "for elem in res.keys():\n",
    "    splitted = elem.split(\"_\")\n",
    "    data = splitted[2]\n",
    "    task = splitted[3]\n",
    "    backbone = splitted[4]\n",
    "    if len(splitted) > 5:\n",
    "        concept = splitted[5]\n",
    "    else: \n",
    "        concept = False\n",
    "    rows.append({\"Concept\": concept, \"Task\": task, \"Concept_source\": backbone, \"Dataset\": data, \"t-RMSE\": res[elem]})\n",
    "\n",
    "#pd.DataFrame(rows).sort_values('size').sort_values('task')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of dictionaries\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = pd.pivot_table(df, values='t-RMSE', index=['Dataset', 'Concept_source', 'Concept'], columns='Task', aggfunc=lambda x: x)\n",
    "\n",
    "# Rename the columns\n",
    "df_pivot.columns = [f'{col}-MAE' for col in df_pivot.columns]\n",
    "# Reset the index\n",
    "df_pivot = df_pivot.reset_index()\n",
    "#df_pivot['Feat. Size'] = [512, , 512, 768, 512, , 512, 768]\n",
    "#df_pivot = df_pivot[[\"Dataset\", \"Backbone\", 'Feat. Size', 'angle-MAE', 'distance-MAE', 'multiangle-MAE', 'multidistance-MAE']].round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_comma = df_pivot[df_pivot['Dataset'] =='comma']\n",
    "categories = [\"[0,9]\", \"[10,19]\", \"[20,19]\", \"[30,40]\"]\n",
    "for i in range(df_pivot_comma['angle-MAE'].shape[0]):\n",
    "    angle = list(pd.Series(df_pivot_comma['angle-MAE'].iloc[i][0]).interpolate(method='linear'))\n",
    "    x_positions = np.arange(len(categories))\n",
    "    plt.plot(x_positions, list(angle))\n",
    "\n",
    "# Relabel the x-axis tick positions with the original categorical values\n",
    "plt.xticks(x_positions, categories)\n",
    "plt.legend(['CLIP', \"Retinanet\"],fontsize=13)\n",
    "plt.xlabel('Steering Angle', fontsize=16)\n",
    "plt.ylabel('MAE', fontsize=16)\n",
    "plt.title(\"Comma2k19\",fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.savefig(\"angle_comma.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
