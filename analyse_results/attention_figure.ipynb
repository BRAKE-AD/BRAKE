{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataloader_comma import CommaDataset\n",
    "from expand_road_events import *\n",
    "from collections import Counter\n",
    "import imageio\n",
    "from model import VTN\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import glob\n",
    "plt.rcParams.update({'font.size': 26}) \n",
    "import os\n",
    "from utils import * \n",
    "import re\n",
    "from vis_utils import * \n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_root = f\"/kaggle/working/ckpts_final_comma_distance_none_True_1_clip\"\n",
    "#find the latest version -G.R.\n",
    "versions = glob.glob(os.path.join(ckpt_root, \"lightning_logs\", \"version_*\"))\n",
    "if not versions:\n",
    "    raise FileNotFoundError(\"None found\")\n",
    "latest_version = max(versions, key=os.path.getmtime)\n",
    "\n",
    "# Find checkpoints in the latest version -G.R.\n",
    "ckpt_files = glob.glob(os.path.join(latest_version, \"checkpoints\", \"*.ckpt\"))\n",
    "if not ckpt_files:\n",
    "    raise FileNotFoundError(\"None found.\")\n",
    "checkpoint_path = max(ckpt_files, key=os.path.getmtime)\n",
    "\n",
    "print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "\n",
    "ckpt = torch.load(checkpoint_path)\n",
    "state_dict = ckpt['state_dict']\n",
    "state_dict = get_regular_ckpt_from_lightning_checkpoint(state_dict)\n",
    "#added this to load the model correctly -G.R.\n",
    "model.load_state_dict(state_dict)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(checkpoint_path, map_location=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ckpt['state_dict']\n",
    "state_dict = get_regular_ckpt_from_lightning_checkpoint(state_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model = model.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(string):\n",
    "    words = string.replace(\"a photo of driving on a highway with\", \"\").replace(\"a photo of\", \"\").replace(\"driving on a highway\", \"\").replace(\"past\", \"\").replace(\"a street with\", \"\").split()  # Split the string into a list of words\n",
    "    result = []\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) <= 33:\n",
    "            current_line += word + \" \"\n",
    "        else:\n",
    "            result.append(current_line.strip())\n",
    "            current_line = word + \" \"\n",
    "    \n",
    "    if current_line:\n",
    "        result.append(current_line.strip())\n",
    "    \n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_dir = \"/kaggle/working/results_images\"\n",
    "gif_dir = \"/kaggle/working/results_images/att\"\n",
    "\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(gif_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from expand_road_events import expand_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataloader_comma import CommaDataset\n",
    "\n",
    "# Create the dataset\n",
    "dataset_comma = CommaDataset(\n",
    "    dataset_type=\"test\",  # o \"val\" o \"train\" depends on what we want plot\n",
    "    use_transform=False,\n",
    "    multitask=\"distance\",  \n",
    "    ground_truth=\"desired\", # Change to True to have all the parameters you use in the plot\n",
    "    dataset_path=\"/kaggle/input/final-hdf5-files\",\n",
    "    dataset_fraction=1.0\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset_comma)} samples\")\n",
    "\n",
    "# Creating the Dataloader\n",
    "dataloader_comma = DataLoader(\n",
    "    dataset_comma,\n",
    "    batch_size=1,  # Keep 1 for plotting\n",
    "    shuffle=False,  # Don't shuffle for plots\n",
    "    num_workers=0,  # 0 for easier debugging\n",
    "    # collate_fn=None  # Use the default collate_fn, NOT the custom one you showed\n",
    ")\n",
    "\n",
    "print(\"DataLoader created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_scenarios\n",
    "\n",
    "scenarios, scenarios_tokens = get_scenarios(\"retinanet\")   #or clip\n",
    "\n",
    "scenarios = list(scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, batch in enumerate(dataloader_comma):\n",
    "    image_array,  vego, angle, distance, g, s, l, seq_key = batch #g= gaspressed, s= brakepressed, l= cruiseenabled\n",
    "        \n",
    "    img = image_array\n",
    "    max_len = 240\n",
    "    # subito dopo il batch unpacking\n",
    "    img, angle, distance, vego = img.to(gpu), angle.to(gpu), distance.to(gpu), vego.to(gpu)\n",
    "    (logits, attns), concepts = model(img, angle, distance, vego, seq_key)\n",
    "    \n",
    "    top5_indices = torch.tensor(concepts.squeeze()).topk(10).indices\n",
    "    s = img.shape\n",
    "    angle, distance, vego, logits, concepts = angle.to(\"cpu\"), distance.to(\"cpu\"), vego.to(\"cpu\"), logits.detach().cpu().to(\"cpu\"), concepts.detach().cpu().to(\"cpu\")\n",
    "    \n",
    "    f = []\n",
    "    inter = []\n",
    "    for i, elem0 in enumerate(top5_indices):\n",
    "        inter = []\n",
    "        for elem in top5_indices[max(i-20, 0):min(i+20,len(top5_indices))]:\n",
    "            l = elem.cpu().numpy().tolist()\n",
    "            if 131 in l:\n",
    "                l.remove(131)\n",
    "            if 55 in l:\n",
    "                l.remove(55)\n",
    "            inter.extend(l)\n",
    "        count_dict = Counter(inter)\n",
    "        # Get the top 5 most occurring numbers\n",
    "        top_5 = count_dict.most_common(3)\n",
    "        intermediate = []\n",
    "        for a in top_5: \n",
    "            intermediate.append(scenarios[a[0]])\n",
    "        f.append(intermediate)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 16))#,gridspec_kw= {'height_ratios': [20, 1]})\n",
    "\n",
    "    plt_idx = 0\n",
    "    for i, image in tqdm(enumerate(img[0][10:120])):\n",
    "    \n",
    "        image_frame = (image).cpu().permute(1, 2, 0)#unorm(image).cpu().permute(1, 2, 0)\n",
    "\n",
    "        # Display the image frame\n",
    "        axes.imshow((np.array(image_frame) * 255).astype(np.uint8))\n",
    "        \n",
    "        #title = '\\n'.join([split_string(\"\\u2022 \" + h) for h in f[i]]) # ORIGINAL\n",
    "        lines = [split_string(h) for h in f[i]]               # senza bullet\n",
    "        expanded = [expand_label(line) for line in lines]     # espandi le frasi\n",
    "        title = '\\n'.join(\"\\u2022 \" + e for e in expanded)    # riaggiungi il bullet\n",
    "                \n",
    "        axes.set_title(title)\n",
    "        axes.set_aspect('equal')\n",
    "        axes.set_xticks([])\n",
    "        axes.set_yticks([])\n",
    "\n",
    "        # Remove borders\n",
    "        axes.spines['top'].set_visible(False)\n",
    "        axes.spines['bottom'].set_visible(False)\n",
    "        axes.spines['left'].set_visible(False)\n",
    "        axes.spines['right'].set_visible(False)\n",
    "\n",
    "        plt.savefig(f\"/kaggle/working/results_images/{i}.png\") #Modified to save images in Kaggle environment\n",
    "\n",
    "    \n",
    "    image_directory = '/kaggle/working/results_images/v2'\n",
    "\n",
    "    # Set the output GIF file path\n",
    "    output_path = lambda x: f'/kaggle/working/results_images/att/attention_comma_{j}.{x}'\n",
    "\n",
    "    # Set the duration (in milliseconds) for each frame in the GIF\n",
    "    frame_duration = 700\n",
    "\n",
    "    # Get a sorted list of image files in the directory\n",
    "    image_files = sorted(glob.glob(f'{image_directory}/*.png'), key=extract_number)  # Adjust the file extension if necessary\n",
    "    \n",
    "    # Create a list to store the frames of the GIF\n",
    "    frames = []\n",
    "\n",
    "    # Iterate over each image file\n",
    "    for image_file in image_files:\n",
    "        # Open the image file\n",
    "        image = Image.open(image_file)\n",
    "\n",
    "        # Add the image to the list of frames\n",
    "        frames.append(image)\n",
    "\n",
    "    # Save the frames as a GIF\n",
    "    #frames[0].save(output_path(\"gif\"), format='GIF', append_images=frames[1:], save_all=True,\n",
    "    #            duration=frame_duration, loop=0)\n",
    "    #imageio.mimsave(output_path(\"mp4\"), frames, fps=4)\n",
    "    \n",
    "    if j > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Secure plotting + marker e zoom\n",
    "# ============================\n",
    "import os, gc, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from scipy.signal import find_peaks\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- Parameters (modify if needed) ----------------\n",
    "outdir = \"/kaggle/working/results_images_safe\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "chunk_size = 8            # number of columns per figure\n",
    "fig_w_per_col = 4.0\n",
    "fig_h = 6.0\n",
    "dpi_save = 200\n",
    "make_gif = True\n",
    "normalize_speed = False   # if True normalizes each speed_graph (useful if curves are very flat)\n",
    "save_model_checkpoint = False\n",
    "checkpoint_path = \"/kaggle/working/model_ckpt.pth\"\n",
    "verbose = True\n",
    "\n",
    "# New parameters for marker/zoom\n",
    "zoom_window = 20          # +/- steps shown around the current frame (adjust here)\n",
    "marker_size = 48\n",
    "marker_color = \"red\"\n",
    "annotate_values = True    # shows numerical value next to the marker\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# helper safe save (draw before save to avoid NoneType renderer)\n",
    "def safe_plot_and_save(fig, png_path, dpi=dpi_save):\n",
    "    try:\n",
    "        fig.canvas.draw()               # builds renderer\n",
    "        fig.savefig(png_path, dpi=dpi, bbox_inches='tight')\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            # fallback: tries without tight bbox/inset\n",
    "            print(f\"[WARN] primary save failed: {e}. Retrying fallback.\")\n",
    "            fig.savefig(png_path, dpi=dpi)\n",
    "        except Exception as e2:\n",
    "            print(f\"[ERROR] save fallback failed: {e2}\")\n",
    "            raise e2\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "\n",
    "# simple split string for titles\n",
    "def split_string(s, max_len=40):\n",
    "    return \"\\n\".join([s[i:i+max_len] for i in range(0, len(s), max_len)])\n",
    "\n",
    "# model device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"[INFO] model device: {device}\")\n",
    "\n",
    "saved_pngs = []\n",
    "\n",
    "# ---------- loop for batch (here we process 1 batch for debugging; remove break for all) ----------\n",
    "for j, batch in enumerate(dataloader_comma):\n",
    "    # your dataloader returns 7 elements: images, vego, angle, distance, gas, brake, cruise\n",
    "    image_array, vego, angle, distance, gas, brake, cruise, seq_key = batch\n",
    "\n",
    "    # move input to model device and do forward\n",
    "    img = image_array.to(device)\n",
    "    angle_dev = angle.to(device); distance_dev = distance.to(device); vego_dev = vego.to(device)\n",
    "\n",
    "    # forward (try/except if model can fail)\n",
    "    with torch.no_grad():\n",
    "        (logits, attns), concepts = model(img, angle_dev, distance_dev, vego_dev, seq_key)\n",
    "\n",
    "    # move to CPU for plotting (img_cpu shape: [B, seq_len, C, H, W])\n",
    "    img_cpu = img.detach().cpu()\n",
    "\n",
    "    # concepts on CPU\n",
    "    concepts_cpu = concepts.detach().cpu()\n",
    "\n",
    "    # attns on CPU (gestisce list/tuple o None)\n",
    "    if attns is None:\n",
    "        attns_cpu = []\n",
    "    elif isinstance(attns, (list, tuple)):\n",
    "        attns_cpu = [a.detach().cpu() for a in attns]\n",
    "    else:\n",
    "        attns_cpu = [attns.detach().cpu()]\n",
    "\n",
    "    # function that produces speed graph from an attention layer (catch errors)\n",
    "    def compute_speed_from_att(atten_cpu):\n",
    "        try:\n",
    "            seq_len_local = atten_cpu.shape[2]\n",
    "            alignment_array = get_aligned_attention(atten_cpu.squeeze().cpu(), seq_len_local)\n",
    "            g = alignment_array.sum(axis=0)[8:-8]\n",
    "            return moving_average(g, 10)\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"[WARN] compute_speed failed: {e}\")\n",
    "            return np.array([])\n",
    "\n",
    "    # compute speed_graphs for the first 3 available elements (otherwise empty array)\n",
    "    speed_graphs = []\n",
    "    for idx in range(3):\n",
    "        if idx < len(attns_cpu):\n",
    "            sg = compute_speed_from_att(attns_cpu[idx][:,:,0:concepts_cpu.shape[1]])\n",
    "        else:\n",
    "            sg = np.array([])\n",
    "        speed_graphs.append(np.array(sg))\n",
    "    sg0, sg1, sg2 = speed_graphs\n",
    "\n",
    "    # diagnostic\n",
    "    if verbose:\n",
    "        for k, sg in enumerate((sg0, sg1, sg2)):\n",
    "            if sg.size:\n",
    "                print(f\"[BATCH {j}] sg{k} min/max/sum: {sg.min():.6f}, {sg.max():.6f}, {sg.sum():.6f}\")\n",
    "            else:\n",
    "                print(f\"[BATCH {j}] sg{k} empty\")\n",
    "\n",
    "    #nes = [split_string(h) for h in f[i]] # senza bullet\n",
    "    #expanded = [expand_label(line) for line in lines] # espandi le frasi \n",
    "    #title = '\\n'.join(\"\\u2022 \" + e for e in expanded) # riaggiungi il bullet\n",
    "\n",
    "    # title composition (top concepts) \n",
    "    topk = torch.tensor(concepts_cpu.squeeze()).topk(5).indices\n",
    "    f = []\n",
    "    for i, _ in enumerate(topk):\n",
    "        inter = []\n",
    "        for elem in topk[max(i-5, 0):min(i+5, len(topk))]:\n",
    "            l = elem.cpu().numpy().tolist()\n",
    "            for bad in (131, 55):\n",
    "                if bad in l:\n",
    "                    l.remove(bad)\n",
    "            inter.extend(l)\n",
    "        cdict = Counter(inter)\n",
    "        top_2 = cdict.most_common(2)\n",
    "        f.append([expand_label(scenarios[a[0]]) for a in top_2])\n",
    "\n",
    "    # optional: build intervention mask for highlighting (use gas/brake/cruise)\n",
    "    try:\n",
    "        intervention_mask = (np.array(gas).astype(bool) | np.array(brake).astype(bool) | (~np.array(cruise).astype(bool)))\n",
    "    except Exception:\n",
    "        intervention_mask = None\n",
    "\n",
    "    # indices of the frames to plot\n",
    "    seq_len = img_cpu.shape[1]\n",
    "    frames_indices = list(range(10, seq_len - 10))\n",
    "    if len(frames_indices) == 0:\n",
    "        print(\"[WARN] nessun frame utile (range 10:-10 vuoto). Skip batch.\")\n",
    "        # pulizia e continua\n",
    "        del img, angle_dev, distance_dev, vego_dev, logits, concepts, attns\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "\n",
    "    def maybe_normalize(arr):\n",
    "        if arr.size == 0: return arr\n",
    "        if not normalize_speed: return arr\n",
    "        mn, mx = arr.min(), arr.max()\n",
    "        if mx - mn < 1e-9: return np.zeros_like(arr)\n",
    "        return (arr - mn) / (mx - mn + 1e-12)\n",
    "\n",
    "    sg0n = maybe_normalize(sg0); sg1n = maybe_normalize(sg1); sg2n = maybe_normalize(sg2)\n",
    "    lengths = [len(x) for x in (sg0n, sg1n, sg2n) if len(x) > 0]\n",
    "    L = max(lengths) if lengths else (len(sg0n) if len(sg0n)>0 else 1)\n",
    "    x_global = np.arange(L)\n",
    "\n",
    "    # -------- plotting chunked & safe saving --------\n",
    "    for chunk_start in range(0, len(frames_indices), chunk_size):\n",
    "        chunk = frames_indices[chunk_start: chunk_start + chunk_size]\n",
    "        ncol = len(chunk)\n",
    "        if ncol == 0:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(2, ncol, figsize=(fig_w_per_col * ncol, fig_h),\n",
    "                                 gridspec_kw={'height_ratios':[3,1]})\n",
    "        axes = np.array(axes)\n",
    "        axes_flat = axes.flatten()\n",
    "\n",
    "\n",
    "        sgs_for_combined = [np.array(a) for a in (sg0n, sg1n, sg2n) if a.size>0]\n",
    "        all_arr = np.concatenate(sgs_for_combined) if len(sgs_for_combined)>0 else np.array([0.0,1.0])\n",
    "        global_min, global_max = float(all_arr.min()), float(all_arr.max())\n",
    "        if abs(global_max - global_min) < 1e-9:\n",
    "            global_min -= 0.5; global_max += 0.5\n",
    "\n",
    "        # precompute combined (interpolato su x_global)\n",
    "        combined = np.zeros(len(x_global))\n",
    "        for a in (sg0n, sg1n, sg2n):\n",
    "            if a.size > 0:\n",
    "                combined[:len(a)] += np.interp(x_global, np.linspace(0, len(a)-1, len(a)), a)\n",
    "        peaks, _ = find_peaks(combined, distance=max(4, len(x_global)//20)) if len(x_global)>1 else ([], {})\n",
    "\n",
    "        for k, frame_idx in enumerate(chunk):\n",
    "            ax_img = axes_flat[k]\n",
    "            ax_plot = axes_flat[k + ncol]\n",
    "\n",
    "            img_tensor = img_cpu[0][frame_idx]\n",
    "            image_frame = img_tensor.permute(1,2,0).numpy()\n",
    "\n",
    "            # image pane\n",
    "            ax_img.imshow((image_frame * 255).astype(np.uint8))\n",
    "            title = '\\n'.join([split_string(\"â€¢ \" + h) for h in (f[frame_idx - 10] if 0 <= frame_idx-10 < len(f) else [])])\n",
    "            ax_img.set_title(title, fontsize=9)\n",
    "            ax_img.axis(\"off\")\n",
    "\n",
    "            # plot heads (interpolando su x_global)\n",
    "            plotted_any = False\n",
    "            heads = [(sg0n, \"#1f77b4\"), (sg1n, \"#ff7f0e\"), (sg2n, \"#2ca02c\")]\n",
    "            linestyles = [\"-\", \"--\", \"-.\"]\n",
    "            linewidths = [2.0, 1.8, 1.8]\n",
    "\n",
    "\n",
    "            head_plots = []\n",
    "            for idx_head, (sg_arr, col) in enumerate(heads):\n",
    "                if sg_arr.size == 0:\n",
    "                    head_plots.append(None)\n",
    "                    continue\n",
    "                sg_plot = np.interp(x_global, np.linspace(0, len(sg_arr)-1, len(sg_arr)), sg_arr) if len(sg_arr)!=len(x_global) else sg_arr\n",
    "                ax_plot.plot(x_global, sg_plot, label=f'Head {idx_head+1}', color=col, linestyle=linestyles[idx_head], linewidth=linewidths[idx_head], alpha=0.95)\n",
    "                head_plots.append(sg_plot)\n",
    "                plotted_any = True\n",
    "\n",
    "            # mark peaks (global)\n",
    "            for p in peaks:\n",
    "                if 0 <= p < len(x_global):\n",
    "                    ax_plot.plot(p, combined[p], marker='o', markersize=4, color='darkred', alpha=0.9)\n",
    "\n",
    "            pos = frame_idx - 10\n",
    "            \n",
    "            pos_clipped = int(np.clip(pos, 0, len(x_global)-1))\n",
    "            if 0 <= pos_clipped < len(x_global):\n",
    "         \n",
    "                for idx_head, sg_plot in enumerate(head_plots):\n",
    "                    if sg_plot is None: \n",
    "                        continue\n",
    "                    y_val = float(np.interp(pos_clipped, x_global, sg_plot))\n",
    "                    ax_plot.scatter(pos_clipped, y_val, color=marker_color, s=marker_size, zorder=5)\n",
    "                    if annotate_values:\n",
    "                        ax_plot.annotate(f\"{y_val:.2f}\", xy=(pos_clipped, y_val), xytext=(3, 3),\n",
    "                                         textcoords=\"offset points\", fontsize=7, color=marker_color)\n",
    "\n",
    "                x_min = max(0, pos_clipped - zoom_window)\n",
    "                x_max = min(len(x_global)-1, pos_clipped + zoom_window)\n",
    "               \n",
    "                y_vals_chunk = []\n",
    "                for sg_plot in head_plots:\n",
    "                    if sg_plot is not None:\n",
    "                        start = x_min; end = x_max+1\n",
    "                        y_vals_chunk.extend(sg_plot[start:end].tolist())\n",
    "                if len(y_vals_chunk) > 0:\n",
    "                    y_min_z = min(y_vals_chunk); y_max_z = max(y_vals_chunk)\n",
    "                    ypad = max(1e-6, (y_max_z - y_min_z) * 0.15)\n",
    "                    ax_plot.set_ylim(y_min_z - ypad, y_max_z + ypad)\n",
    "                ax_plot.set_xlim(x_min, x_max)\n",
    "\n",
    "           \n",
    "            if (intervention_mask is not None) and (len(intervention_mask) == img_cpu.shape[1]):\n",
    "                seg_on=False; start=None\n",
    "                for t_i, val in enumerate(intervention_mask):\n",
    "                    if val and not seg_on: seg_on=True; start=t_i\n",
    "                    if (not val) and seg_on: seg_on=False; end=t_i; \n",
    "                    if 'end' in locals():\n",
    "                        ax_plot.add_patch(patches.Rectangle((start, ax_plot.get_ylim()[0]), end-start, ax_plot.get_ylim()[1]-ax_plot.get_ylim()[0], color='magenta', alpha=0.06, linewidth=0))\n",
    "                if seg_on:\n",
    "                    end = len(intervention_mask)-1\n",
    "                    ax_plot.add_patch(patches.Rectangle((start, ax_plot.get_ylim()[0]), end-start, ax_plot.get_ylim()[1]-ax_plot.get_ylim()[0], color='magenta', alpha=0.06, linewidth=0))\n",
    "\n",
    "           \n",
    "            if plotted_any:\n",
    "                ax_plot.legend(fontsize=8, loc='upper right')\n",
    "            else:\n",
    "                ax_plot.text(0.5, 0.5, \"No attention data\", ha='center', va='center', fontsize=9, color='gray')\n",
    "                ax_plot.set_ylim(0, 1)\n",
    "\n",
    "            ax_plot.set_xlabel(\"Sequence position\", fontsize=8)\n",
    "            ax_plot.set_ylabel(\"Attention activation\", fontsize=8)\n",
    "            ax_plot.tick_params(axis='both', which='major', labelsize=7)\n",
    "            ax_plot.grid(axis='y', linestyle=':', linewidth=0.5, alpha=0.6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        png_path = os.path.join(outdir, f\"batch{j}_chunk{chunk_start}.png\")\n",
    "        safe_plot_and_save(fig, png_path, dpi=dpi_save)\n",
    "        saved_pngs.append(png_path)\n",
    "\n",
    " \n",
    "    if make_gif and saved_pngs:\n",
    "        try:\n",
    "            frames = [Image.open(p) for p in saved_pngs]\n",
    "            gif_path = os.path.join(outdir, f\"attention_batch{j}.gif\")\n",
    "            frames[0].save(gif_path, format='GIF', append_images=frames[1:], save_all=True, duration=500, loop=0)\n",
    "            for im in frames: im.close()\n",
    "            print(f\"[INFO] GIF creata: {gif_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] creazione GIF fallita: {e}\")\n",
    "\n",
    "   \n",
    "    del img, angle_dev, distance_dev, vego_dev, logits, concepts, attns\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  \n",
    "    if save_model_checkpoint:\n",
    "        try:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"[INFO] model saved in {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] save model failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"[INFO] model deleted from memory.\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "  \n",
    "    print(\"[DONE] saved pngs:\", saved_pngs)\n",
    "    if saved_pngs:\n",
    "        from IPython.display import display\n",
    "        display(Image.open(saved_pngs[0]))\n",
    "\n",
    "    break   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
