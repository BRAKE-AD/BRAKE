{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from dataloader_comma import *\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git\n",
    "#!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dataloader_comma import *\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "from utils import get_scenarios\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "multitask = \"distance\"\n",
    "dataset_type = 'train'\n",
    "dataset = CommaDataset(dataset_type=dataset_type, multitask=multitask, return_full=True)\n",
    "loader = DataLoader(dataset, batch_size=1, num_workers=5)\n",
    "\n",
    "##CHANGE PATH\n",
    "save_path = '/kaggle/working/'\n",
    "dfs = []\n",
    "scenarios, scenarios_tokens = get_scenarios(\"retinanet\")   #or clip\n",
    "scenarios = list(scenarios)\n",
    "text = clip.tokenize(scenarios).to(device)\n",
    "scenarios_tokens = scenarios_tokens.to(device)\n",
    "p= '/kaggle/input/scenarios/'\n",
    "\n",
    "feats = []\n",
    "similarities = []\n",
    "similarities_labels = []\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(scenarios_tokens)\n",
    "for elem in os.listdir(p): \n",
    "    if \".txt\" in elem: continue\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(p + elem)\n",
    "    img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
    "    img = img.to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(img)\n",
    "    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    feats.append(image_features)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    with open((p + elem).replace(\".png\", \".txt\"), 'r') as f:\n",
    "        # Read the contents of the file as a string\n",
    "        dict_str = f.read()\n",
    "    similarities_labels.append(list(eval(dict_str).values()))\n",
    "    similarities.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = torch.concatenate(feats)\n",
    "stacked = torch.stack([torch.tensor(list(similarities_labels)) for elem in range(240)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(image_array, feats, text_features, stacked, intervention):\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_array[intervention.squeeze().tolist()])\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    img_similarity = (100.0 * image_features @ feats.T).softmax(dim=-1)\n",
    "    text_similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    img_sim, max_indices = torch.max(img_similarity, dim=1)\n",
    "    s = stacked[intervention.squeeze().tolist()]\n",
    "    selected_indices = s[torch.arange(len(s)), max_indices.cpu()].squeeze(dim=1)\n",
    "    similarities = (1-img_sim).unsqueeze(dim=1).cpu() * text_similarity.cpu() + (img_sim).unsqueeze(dim=1).cpu() * selected_indices.cpu()\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_reinstate = []\n",
    "top_5_intervene_before = []\n",
    "top_5_intervene_after = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j, batch in tqdm(enumerate(loader)):\n",
    "        image_array, vego, angle, distance, gas, brake, ccenabled = batch\n",
    "        image_array = image_array.squeeze().to(device)\n",
    "        intervention = ccenabled.squeeze()\n",
    "\n",
    "        #switch_indices = np.where(np.diff(intervention) == 1)[0] + 1\n",
    "        intervention_after = get_reduced_sample(ccenabled, reinstate=False, after=True)\n",
    "        intervention_before = get_reduced_sample(ccenabled, reinstate=False, after=False)\n",
    "        \n",
    "        if intervention_before.sum() != 0: \n",
    "            img_sim_intervention_before = get_similarity(image_array, feats, text_features, stacked, intervention_before)\n",
    "            values, indices = img_sim_intervention_before.float().topk(5, dim=1)\n",
    "            indices, idx = torch.sort(indices,dim=1)\n",
    "            values, indices = indices.topk(1, dim=0)\n",
    "            top_5_intervene_before.extend(values.tolist())\n",
    "\n",
    "            img_sim_intervention_after = get_similarity(image_array, feats, text_features, stacked, intervention_after)\n",
    "            values, indices = img_sim_intervention_after.float().topk(5, dim=1)\n",
    "            indices, idx = torch.sort(indices,dim=1)\n",
    "            values, indices = indices.topk(1, dim=0)\n",
    "            top_5_intervene_after.extend(values.tolist())\n",
    "\n",
    "        '''\n",
    "        #reinstate = get_reduced_sample(ccenabled, reinstate=True)\n",
    "        # if reinstate.sum() != 0: \n",
    "            img_sim_reinstate = get_similarity(image_array, feats, text_features, stacked, reinstate)\n",
    "            values, indices = img_sim_reinstate.float().topk(5,dim=1)\n",
    "            top_5_reinstate.extend(indices.tolist())\n",
    "        #values, indices = similarity[0].topk(5)\n",
    "        \n",
    "        \n",
    "        # Print the result'''\n",
    "        '''print(\"\\nTop predictions:\\n\")\n",
    "        for value, index in zip(values, indices):\n",
    "            print(f\"{scenarios[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_intervene_after_sq = torch.tensor(top_5_intervene_after).reshape((len(top_5_intervene_after)* len(top_5_intervene_after[1]), 1)).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_intervene_before_sq = torch.tensor(top_5_intervene_before).reshape((len(top_5_intervene_before)* len(top_5_intervene_before[1]), 1)).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(top_5_intervene_after_sq, columns=[\"scenario\"])\n",
    "df['scenario'] = df.scenario.apply(lambda x: scenarios[x])\n",
    "df['c'] = 1\n",
    "df.groupby(\"scenario\").sum().sort_values(by=\"c\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(top_5_intervene_before_sq, columns=[\"scenario\"])\n",
    "df['scenario'] = df.scenario.apply(lambda x: scenarios[x])\n",
    "df['c'] = 1\n",
    "df.groupby(\"scenario\").sum().sort_values(by=\"c\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(top_5_intervene).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_intervene_sq = torch.tensor(top_5_intervene).reshape((843* 5, 1)).squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interv = pd.DataFrame(top_5_intervene_sq, columns=[\"scenario\"])\n",
    "df_interv['scenario'] = df_interv.scenario.apply(lambda x: scenarios[x])\n",
    "df_interv['c'] = 1\n",
    "df_interv.groupby(\"scenario\").sum().sort_values(by=\"c\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/kaggle/working/explanation_intervention_cc/'\n",
    "top3 = []\n",
    "for elem in os.listdir(p):\n",
    "    if \".json\" not in elem: continue\n",
    "    with open(p + elem, 'r') as f:\n",
    "    # Load the data from the file into a Python dictionary:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(list(data.items()), columns=['Text', 'Probability'])\n",
    "    df[\"Probability\"] = df[\"Probability\"].apply(lambda x: float(x))\n",
    "    df = df[df[\"Text\"] != \"a picture of cars driving during the day\"]\n",
    "    df = df[df[\"Text\"] != \"a picture of cars driving in the dark\"]\n",
    "    top3.extend(df.sort_values(\"Probability\", ascending=False)[0:5].Text.tolist())\n",
    "df = pd.DataFrame(top3)\n",
    "df['count'] = 1\n",
    "df.groupby(0).sum().sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dataloader_comma import *\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "multitask = \"distance\"\n",
    "dataset_type = 'train'\n",
    "dataset = CommaDataset(dataset_type=dataset_type, multitask=multitask, return_full=True)\n",
    "loader = DataLoader(dataset, batch_size=1, num_workers=5)\n",
    "save_path = '/kaggle/working/explanation_intervention_cc/'\n",
    "\n",
    "text = clip.tokenize(scenarios).to(device)\n",
    "scenarios_tokens = scenarios_tokens.to(device)\n",
    "with torch.no_grad():\n",
    "    for j, batch in tqdm(enumerate(loader)):\n",
    "        image_array, vego, angle, distance, gas, brake, ccenabled, seq_key = batch\n",
    "        intervention = ccenabled.squeeze()\n",
    "\n",
    "        #switch_indices = np.where(np.diff(intervention) == 1)[0] + 1\n",
    "        switch_indices = np.where(np.diff(intervention.int()) == -1)[0] + 1\n",
    "\n",
    "        intervention = np.zeros_like(intervention.squeeze(), dtype=bool)\n",
    "        intervention[switch_indices] = True\n",
    "\n",
    "        if intervention.sum() == 0: \n",
    "            continue\n",
    "        \n",
    "\n",
    "        images = image_array.squeeze()[intervention.squeeze().tolist()]\n",
    "        if images.shape[0] == 0: \n",
    "            del image_array, vego, angle, distance, gas, brake\n",
    "            continue\n",
    "\n",
    "        \n",
    "        img = images\n",
    "        s = img.shape#[batch_size, seq_len, h,w,c]\n",
    "        logits_per_image, logits_per_text = model(img.to(device), scenarios_tokens.to(device))\n",
    "        probs = logits_per_image.detach()\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().detach()\n",
    "        df = pd.DataFrame([scenarios[i.item()] for i in probs.argmax(dim=-1)])\n",
    "        #df.to_csv(f'{save_path}/{j}.csv')\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(img.permute(1,2,0).int())\n",
    "            fig.suptitle(str(df.iloc[i]))\n",
    "            name = save_path + f\"{i}_{j}\" + \"_image.png\"\n",
    "            result_dict = {scenarios[k]: str(probs[i][k].item()) for k in range(len(scenarios))}\n",
    "            with open(name.replace('_image.png', \".json\"), 'w') as f:\n",
    "                json.dump(result_dict, f)\n",
    "            fig.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
